---
title: "Import and Clean Omni Data"
output:
  html_document:
    toc: true
    toc_float: true
    theme: paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.path = 'figures/',
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  fig.width = 10, fig.height = 3,
  cache = FALSE)
```

---

```{r}
library(dplyr)
library(purrr)
library(tidyr)
library(openair)
library(ggplot2)
library(readr)
library(googlesheets4)
library(lubridate)
library(data.table)
library(kableExtra)
```

```{r define_variables}
#define the homes that will be used later in analysis
homes_no5 <- c(
  map(c(1:4,6:9), function(x) paste0('00', x)),
  map(c(10:16), function(x) paste0('0', x))
  )%>% unlist()


##For plot labeling pruposes
xsmall <- 6
small <- 15
medium <- 20
large <- 26

# color blind pallette
# check http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette
cbbPalette <- c('black' = "#000000",
                'light_orange' = "#E69F00",
                'light_blue' = "#56B4E9",
                'green' = "#009E73",
                'yellow' = "#F0E442",
                'dark_blue' = "#0072B2",
                'dark_orange' = "#D55E00",
                'purple' = "#CC79A7"
                )
# define colors for room-comparisons
location_colors <- c(cbbPalette['green'],
                   cbbPalette['purple'],
                   cbbPalette['light_orange'],
                   cbbPalette['dark_blue'],
                   cbbPalette['dark_orange']
                   )%>% unname()

 location_shapes <- c(16, 17, 15, 7, 13) 


location_breaks <- c('living', 'bedroom', 'kitchen', 'garage', 'outdoor')

location_labels <- c('Living Room', 'Bedroom', 'Kitchen', 'Garage', 'Outdoor')



```


# Clean minutely data


```{r import_all_data, eval = FALSE}

omni_data <- read_rds(paste0('./csv_created_article/pre-calibration/omni_all_locations_raw.rds'))

```

# only use data up to June 2021
```{r censor_date, eval = FALSE}
omni_data <- omni_data %>% filter(datetime<ymd_hms('2021-06-01 00:00:00'))

```


# find and remove (or average) duplicates
# and create rds file of uncalibrated data

```{r clean_data, eval = FALSE}

# clean duplicates from sensor overlaps----------
 
 # remove home 003 outdoor from March 21 to March 23
# when temp was above 15 (from overlap with living room sensor)
omni_data_clean <- omni_data %>%
    filter(!(home=='003' & location == 'outdoor'&
             between(datetime, ymd_hms('2021-03-21 17:59:00'),
                     ymd_hms('2021-03-23 23:59:59')) &
                     temp >15))

  # average dupicates in home 009 living due to sensor overlap
 a <- omni_data_clean %>%
    filter(!(home=='009' & location == 'living'))


b <- omni_data_clean %>%
  filter(home=='009', location == 'living') %>%
    group_by(datetime, home, location) %>% 
    summarise_if(is.numeric, list(mean), na.rm = TRUE, .groups = 'drop')

omni_data_clean <- bind_rows(a,b)

  # average dupicates in home 003 living due to sensor overlap
 a <- omni_data_clean %>%
    filter(!(home=='003' & location == 'living'))


b <- omni_data_clean %>%
  filter(home=='003', location == 'living') %>%
    group_by(datetime, home, location) %>% 
    summarise_if(is.numeric, list(mean), na.rm = TRUE, .groups = 'drop')


omni_data_clean <- bind_rows(a,b)
 


# delete duplicates from pm10_est addition------------

# from april 7 to april 15 there are some nearly identical values

 a <- omni_data_clean %>%
    filter(!between(datetime, ymd_hms('2021-04-07 00:00:00'),
                     ymd_hms('2021-04-15 23:59:59')))


b <- omni_data_clean %>%
  filter(between(datetime, ymd_hms('2021-04-07 00:00:00'),
                     ymd_hms('2021-04-15 23:59:59')))


b <- b %>%
    group_by(datetime, home, location) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE, .groups = 'drop')


# merge and delete pm10_est col
omni_data_clean <- bind_rows(a,b) %>%
  select(-pm10_est)


# average duplicates from daylight savings------------

# use data.table to count duplicates
b_dt <- omni_data_clean %>%  data.table()

a <-  (b_dt[, count:= .N,
            by = omni_data_clean %>% select(c(datetime, home, location)) ]) %>%
  as.data.frame()

# unique rows
unique<- a %>%
  filter(count == 1) %>%
  select(-count)

# duplicate rows
duplicates<- a %>%
  filter(count >1) %>%
  select(-count)

# average duplicates that are from daylight savings
avg_daylight <-  duplicates %>%
  filter(between(datetime, ymd_hms("2020-10-31 23:59:00"),
                                ymd_hms("2020-11-01 01:59:00"))) %>%
  group_by(datetime, home, location) %>% 
  summarise_if(is.numeric, mean) %>%
  ungroup()


duplicates_no_daylight <-  duplicates %>%
  filter(!between(datetime, ymd_hms("2020-10-31 23:59:00"),
                                ymd_hms("2020-11-01 01:59:00"))) 
  

  # combine rows into dataframe without duplicates
 omni_data_clean <- bind_rows(unique,duplicates_no_daylight,avg_daylight)
  
 # test to ensure no duplicates are remaining (should be no rows)----------
 
 if(nrow(duplicates_no_daylight)>0) {
stop('duplicates outside of daylight savings time exist')}


 # omit unrealistic temperatures-------------

  omni_data_clean <- omni_data_clean %>%
  mutate(temp = ifelse(temp > 100, NA, temp))

 
 # copy outdoor data for outdoor duplex units ----------------
 omni_data_clean <- omni_data_clean %>%
   bind_rows(
     omni_data_clean %>%
               filter(home == '014' & location == 'outdoor') %>%
               mutate(home = '013'),
     omni_data_clean %>%
               filter(home == '015' & location == 'outdoor') %>%
               mutate(home = '016')
     )

  # write data and backup------------------- 

write_rds(omni_data_clean, paste0('./csv_created_article/pre-calibration/omni_all_locations_pre-cal_', Sys.Date(), '.rds'))

write_rds(omni_data_clean, paste0('./csv_created_article/pre-calibration/omni_all_locations_pre-cal.rds'))
 
```


# create data of multiple resolutions from cleaned data
# and create rds files

```{r hourly_data_rds, eval = FALSE}

omni_hourly_data <- omni_data_clean %>%
  group_by(datehour = floor_date(datetime, unit = 'hour'),
           home, location) %>%
  summarise_if(is.numeric, list(mean), na.rm = TRUE) %>%
  ungroup()

# make csv and backup------------------------

write_rds(omni_hourly_data, paste0('./csv_created_article/pre-calibration/omni_hourly_data_pre-cal_',
Sys.Date(),'.rds'))

write_rds(omni_hourly_data, paste0('./csv_created_article/pre-calibration/omni_hourly_data_pre-cal.rds'))

```

```{r daily_data_rds, eval = FALSE}

  omni_daily_data <- omni_data_clean %>%
  group_by(dateday = floor_date(datetime, unit = 'day'),
           home, location) %>%
  summarise_if(is.numeric, list(mean), na.rm = TRUE) %>%
  ungroup()

# make csv and backup------------------------
write_rds(omni_daily_data, paste0('./csv_created_article/pre-calibration/omni_daily_data_pre-cal_',
Sys.Date(),'.rds'))

write_rds(omni_daily_data, paste0('./csv_created_article/pre-calibration/omni_daily_data_pre-cal.rds'))

```

# calibrate data with filter concentrations

```{r import_uncalibrated_data}
omni_data_precal <- 
  # read in uncalibrated data
  read_rds(paste0('./csv_created/pre-calibration/omni_all_locations_pre-cal.rds')) %>%
  filter(home %in% homes_no5)

```



```{r import_filter_data}
filter_data <- read_rds('../output_thesis/pm25/pm25.rds')
```

* count how many upas samples did not have enough data (not including blanks)
```{r }
# filter values that were not blanks, but had NA mass values
filters_invalid <- filter_data %>%
  filter(location != 'blank' & is.na(conc))

#number of invalid samples
nrow(filters_invalid)

# number of valid samples
filter_data %>% filter(location != 'blank') %>% nrow()

#fraction of invalid
nrow(filters_invalid) /  filter_data %>% filter(location != 'blank') %>% nrow()

```

# create correction factor

* amount of filter-sensor pairs will be more than amount of filters
* because living room filters were duplicated to pair to all indoor rooms
```{r find_correction_ratios_for_all_sensors}

# add correciton factor to real-time pm2.5 data
blank_data <- filter_data %>%
  filter(location == 'blank'& id_home %in% homes_no5) %>%
  select(visit, id_home, d_mass_blank = d_mass)

filter_data_correct <- filter_data %>%
    filter(location != 'blank'& id_home %in% homes_no5) %>%
  left_join(blank_data, by = c('visit', 'id_home')) %>%
  mutate(conc_correct = if_else(duration >= 24, ((d_mass-d_mass_blank)*1000)/(volume/1000),
                                NA_real_))%>%
  # duplicate results for livingroom to all indoor rooms
  select(id_home, location, visit, conc_correct) %>%
  pivot_wider(names_from = c(location), values_from = conc_correct) %>%
  mutate(kitchen = living, bedroom = living) %>%
  pivot_longer(cols = -c(id_home, visit) , names_to = 'location',
               values_to = 'conc_correct')
  
# make dataframe of corrrection factors
grav_cf_df <- omni_data_precal %>%
  #calculate TWA for pm2.5 for each home/location
  group_by(home, location) %>%
  summarise_at(vars(pm25), funs(twa = mean, n_hours = n()/(60/5),
                                )) %>%
  ungroup() %>%
  # match filter-based concentration to TWA for each home/location
  left_join(filter_data_correct, by = c('home' = 'id_home', 'location')) %>%
  mutate(
    grav_cf = twa/conc_correct
         # # fill in NA values with median correction factor
         # grav_cf = ifelse(is.na(conc_correct), median(grav_cf, na.rm = TRUE),
         #                  grav_cf)
  )

```

* check between rooms and ensure same calibration value can be used for all indoor rooms
```{r}
# a <- grav_cf_df %>% filter(location %in% c('living', 'kitchen', 'bedroom'))
# ggplot(a, aes(y = twa, x = location))+
#   geom_boxplot()

```


*look at correction factor spread (for median)
```{r}
# check spread of correction factor values
ggplot(grav_cf_df, aes(y = grav_cf))+
  geom_boxplot()

```

* count negative CF values from pairwise method
```{r}
 grav_cf_df %>% filter(!is.na(grav_cf) & grav_cf < 0) %>%nrow()

```
 
* out of ___ CF values
```{r}
 grav_cf_df %>% filter(!is.na(grav_cf)) %>%nrow()
```



* after omitting negatives...

```{r, fig.height=3, fig.width=1.5}

grav_cf_df_clean <- grav_cf_df %>%
  filter(!is.na(grav_cf) & grav_cf >0)


# check spread of correction factor values
ggplot(grav_cf_df_clean, aes( x = 1, y = grav_cf))+
  geom_boxplot( outlier.shape = NA)+
  geom_jitter( aes( color = location, shape = location), width = 0.15)+
  theme_classic()+
  theme(
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks.x = element_blank()
  )+
  ylab('Gravimetric Correction Factor')+
  scale_color_manual(
    breaks = location_breaks,
    values = location_colors,
    labels = location_labels,
    name = 'Monitoring Location'
  )+
    scale_shape_manual(
    breaks = location_breaks,
    values = location_shapes,
    labels = location_labels,
    name = 'Monitoring Location'
  )+
  theme(
    legend.position = 'none'
  )


```


* summary stats of pairwise CF values (without negative values)
```{r}
# min
grav_cf_df_clean %>% pull(grav_cf) %>% min()

# max
grav_cf_df_clean %>% pull(grav_cf) %>% max()

```


*look at regression of TWA to filter values without omitting outliers

```{r}
model <- lm("twa~conc_correct+0", data = grav_cf_df_clean)

# print model results
broom::tidy(model, conf.int = TRUE) %>%
  kable(digits = c(NA, 2, 2, 2, 6 , 2, 2), caption = "OMNI vs. Filter PM2.5") %>%
  kable_paper(bootstrap_options = "striped", full_width = F)
```


*omit outliers for regression again
```{r}
# find limits within 2 sds of mean
mean_cf <- grav_cf_df_clean %>% pull(grav_cf) %>% mean()

sd_cf <- grav_cf_df_clean %>% pull(grav_cf) %>% sd()

grav_cf_df_omit <-  grav_cf_df_clean %>%
  filter(grav_cf > mean_cf-2.5*sd_cf & grav_cf < mean_cf+2.5*sd_cf)

# amount of values omitted
(   nrow(grav_cf_df_clean) -  nrow(grav_cf_df_omit) )

# fraction ""
(   nrow(grav_cf_df_clean) -  nrow(grav_cf_df_omit) ) /   nrow(grav_cf_df_clean)
```

*look at regression of TWA to filter values after omitting outliers

```{r}
model <- lm("twa~conc_correct+0", data = grav_cf_df_omit)

# print model results
broom::tidy(model, conf.int = TRUE) %>%
  kable(digits = c(NA, 2, 2, 2, 6 , 2, 2),
        caption = "OMNI vs. Filter PM2.5, outliers omitted") %>%
  kable_paper(bootstrap_options = "striped", full_width = F)
```


# Plot real-time vs filter values with legend

```{r fig_omni_vs_filter, fig.width=6, fig.height=4.5}
model_x_values <- seq(0,110,1)

  # add boundarie for inset plot
x_inset <- 21
y_inset <-21

# make predictions based on model
predicted_df <- data.frame(twa_pred = model$coefficients*model_x_values,
                   conc_correct = model_x_values)

ggplot(grav_cf_df_clean, aes(x = conc_correct, y = twa)) +
  geom_point(aes(
    color = location, shape = location
    )) +
  #geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  theme_bw() +
  geom_abline(slope = 1, intercept = 0, linetype = 'dashed') +
  geom_line(data = predicted_df, aes( y = twa_pred
                                      # linetype = 'model'
                                      ),
            linetype = 'solid')+
  # add boundarie for inset plot
  geom_rect(xmin = 0, xmax = x_inset, ymin = 0, ymax = y_inset, fill = NA,
            color = cbbPalette['light_blue'])+
  scale_y_continuous(expand = c(0,0), limits = c(0,110))+
  scale_x_continuous(expand = c(0,0), limits = c(0,110))+
    scale_color_manual(
    breaks = location_breaks,
    values = location_colors,
    labels = location_labels,
    name = 'Monitoring Location'
  )+
      scale_shape_manual(
    breaks = location_breaks,
    values = location_shapes,
    labels = location_labels,
    name = 'Monitoring Location'
  )+
  ylab(expression(paste('IAQ Sensor TWA PM'[2.5], ', ug/m'^3)))+
  xlab(expression(paste('Gravimetric PM'[2.5], ', ug/m'^3)))+
          theme(
      axis.title = element_text(size = small+2),
      axis.text = element_text(size = small+2),
      legend.text = element_text(size = small+2),
      legend.title = element_text(size = small+2),
              )

# scale_linetype_manual(
  #   values = c('dashed', 'solid'),
  #   # breaks = c('dashed', 'solid'),
  #   labels = c('1 to 1', 'Model')
  # )


cbbPalette
  
# # without filter "outliers"
# 
# a %>% grav_cf_df_clean %>% filter(conc_correct<50)
# 
# model <- lm("twa~conc_correct+0", data = a)

```


# Inset plot

```{r fig_omni_vs_filter, fig.width=6, fig.height=4.5}

model_x_inset <- seq(0,x_inset,1)

# make predictions based on model
predicted_inset <- data.frame(twa_pred = model$coefficients*model_x_inset,
                   conc_correct = model_x_inset)
ggplot(grav_cf_df_clean, aes(x = conc_correct, y = twa)) +
  geom_point(aes(
    color = location, shape = location
    ),
    size = 5) +
  #geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  theme_bw() +
  geom_abline(slope = 1, intercept = 0, linetype = 'dashed', size = 2) +
  geom_line(data = predicted_inset, aes( y = twa_pred
                                      # linetype = 'model'
                                      ),
            linetype = 'solid', size = 2)+
  scale_y_continuous(expand = c(0,0), limits = c(0,y_inset))+
  scale_x_continuous(expand = c(0,0), limits = c(0,x_inset))+
    scale_color_manual(
    breaks = location_breaks,
    values = location_colors,
    labels = location_labels,
    name = 'Monitoring Location'
  )+
      scale_shape_manual(
    breaks = location_breaks,
    values = location_shapes,
    labels = location_labels,
    name = 'Monitoring Location'
  )+
          theme(
      axis.title = element_blank(),
      axis.text = element_text(size = small+14),
      legend.position = 'none'
              )



```


* calibrate with decided method of correction factor
```{r}
# find the median value of correction factors (exclude negatives)
# and use that to correct all households
grav_cf_median <- grav_cf_df_clean %>%
  pull(grav_cf) %>% median()

omni_data_cal <- omni_data_precal %>%
  mutate(pm25 = pm25/grav_cf_median)

```

# Add in energy cluster column

```{r}
daily_energy_df <- read_csv('../sense/csv_created_sense/daily_energy_data_clustered.csv')

omni_data_cal <- omni_data_cal %>%
  mutate(dateday = floor_date(datetime, unit = 'day')%>%as.Date())%>%
  left_join(daily_energy_df, by = c('dateday', 'home'))

```


```{r}
# make rds file and backup-----------
write_rds(omni_data_cal, paste0('./csv_created/omni_calibrated_',
Sys.Date(),'.rds'))

write_rds(omni_data_cal, paste0('./csv_created/omni_calibrated.rds'))
```

```{r hourly_cal_data_rds}

omni_hourly_data <- omni_data_cal %>%
  select(-heat, -ac) %>%
  group_by(datehour = floor_date(datetime, unit = 'hour'),
           home, location) %>%
  summarise_if(is.numeric, list(mean), na.rm = TRUE) %>%
  ungroup() %>%
  mutate(dateday = floor_date(datehour, unit = 'day')%>%as.Date())%>%
  left_join(daily_energy_df, by = c('dateday', 'home'))

# make rds file and backup-----------------
write_rds(omni_hourly_data, paste0('./csv_created/omni_hourly_calibrated_',
Sys.Date(),'.rds'))

write_rds(omni_hourly_data, paste0('./csv_created/omni_hourly_calibrated.rds'))

```

```{r daily_cal_data_rds}

  omni_daily_data <- omni_data_cal %>%
    select(-heat, -ac) %>%
  group_by(dateday = floor_date(datetime, unit = 'day'),
           home, location) %>%
  summarise_if(is.numeric, list(mean), na.rm = TRUE) %>%
  ungroup()%>%
  mutate(dateday = dateday %>%as.Date())%>%
  left_join(daily_energy_df, by = c('dateday', 'home'))

# make rds file and backup-----------------
write_rds(omni_daily_data, paste0('./csv_created/omni_daily_calibrated_',
Sys.Date(),'.rds'))

write_rds(omni_daily_data, paste0('./csv_created/omni_daily_calibrated.rds'))


```

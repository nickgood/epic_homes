---
title: "Representativeness"
output:
  html_document:
    toc: true
    toc_float: true
    theme: paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.path = 'figures/',
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  fig.width = 10, fig.height = 3,
  cache = FALSE)
```

```{r libraries, include=FALSE}
library(dplyr)
library(purrr)
library(tidyr)
library(openair)
library(ggplot2)
library(readr)
library(googlesheets4)
library(lubridate)
library(gridExtra)
library(runner) # for moving window functions
library(ggpubr) # for tesing log-normality
# library(cubature) # for alternative method of integrating kld function
# library(entropy) # for KLD function
# library(fitdistrplus) # for fitting distribution to data NOTE, MASS::select CONFLICTS WITH dplyr::select

```


```{r import_omni_minute_data}
omni_data<- read_csv('./csv_created/omni_all_locations.csv')


zero_test <- omni_data %>%
  group_by(home, location) %>%
  summarize(pm25_zeros = sum(pm25 == 0), pm25_n = n()) %>%
  mutate(pm25_zero_pct = pm25_zeros/pm25_n*100) %>%
  ungroup()

```



```{r functions_misc}

#Function to only display 3 significant figures (for tables)
signif3 <- function(x){
  signif(x, digits = 3)
}

##Define a char vector of home numbers using a number vector,
##ex: x <- home.list(c(1:15)) for homes 1-15
threedig <- function(x) {
  if (nchar(x) == 1) {a <- paste0('00', x)
  return(a)
  }
  
  if (nchar(x) == 2) {a<- paste0('0', x)
  return(a)
  }
  else return(a)
}

home.list <- function(x) sapply(x, threedig)


#function to make density plot of variable x (a column)
dens.plot <- function(data, metric, rm) {
  
ggdensity(data %>%
            filter(room == rm) %>%
            pull(all_of(metric)), 
          main = paste("Density plot of", rm, metric),
          xlab = metric)
}




```

```{r function_time_master}
test_p <- runif(100, min=1, max=22) %>% floor()
length(unique(test_p))

test_q <- runif(100, min=1, max=100) %>% floor()
length(unique(test_q))

entropy::KL.plugin(test_p,
                   test_q,
                   unit = 'log')

kld <- function(p,q){
  lapply(1:length(p), function(x){
         p <- p[x]/sum(p)
         q <- q[x]/sum(q)
         
         p*log(p/q)
         }
         ) %>% unlist() %>% sum()
}


# give a row of NA values with identifers to help with identifying
# causes of errors in function
na.result <- function(error, season = NA, sample_days = NA, samp_avail = NA) {
  tibble(
  'coeff' = NA,
  'high_val' = NA,
  'low_val' = NA,
  'sample_length' = sample_days, # sample_length,
  'monitor_season' = season, # period_label,
  'n_samp_avail' = samp_avail, # sample_available, # amount of samples with sufficient data
  'error' = error
)
}




# calculate kld for continuous probability distributions

# define variables for testing-----------------------
date_ranges_entire <- list(c('2020-11-01','2020-11-30'))
tested_sample_sequence <- seq(3,28,4)
sample_days_min <- 3
home_num <- '004'
location_type <- 'living'
data <- omni_data
monitor_season <- date_ranges_entire[1]
metric <- 'pm25'
days <- 16
days2 <- 16
all_time <- TRUE

# remove variables after done testing
rm(tested_sample_sequence,
   date_ranges_entire,
sample_days_min,
home_num,
monitor_season,
monitor_period,
metric,
days,
days2,
all_time,
data,
data_season,
data_year,
entire,
date_col_season,
date_col_year,
all_dates_season,
all_dates_year
)



# entropy function minutely data ----------------------------




# function to calculate scaled entropy for a house
# during multiple different time periods
scaled.entropy.table.time <- function(
  home_num, metric,
  
  # date range of entire monitoring period
  # in the form: list(
  #                    c('YYYY-MM-DD', 'YYYY-MM-DD'),
  #                    c('YYYY-MM-DD', 'YYYY-MM-DD')
  #                  )
  # data in start and end day included
  date_ranges_entire,
  tested_sample_sequence, # sequence of tested short sampling period lengths (in days)
  location_type, # sensor location (living, bedroom, or kitchen)
  all_time = FALSE, # if TRUE, compare each short sampling period to entire monitoring year of home
  sample_days_min, # amount of days in shortest theoretical sample
  
  data = omni_data) {
  
  output <<- na.result(NA) # clear error output in case it was triggered previously
  
  if(metric %in% c('pm25', 'voc')) {
    # define LOD of metric to be 1/2 minimum detected (non-zero) value
    LOD <- data %>%
      filter(!!sym(metric)!=0) %>%
      pull(all_of(metric))%>%
      min()
  }
  

  # function to find scaled_entropy for samples in a given monitor season

  scaled.entropy.season <- function(monitor_season) {


    # must unlist the listed range that is used in lapply funnction
    monitor_season <- unlist(monitor_season)

    # stop if monitoring season is empty
    if(is_empty(monitor_season)) {
      output <<- na.result('likely_no_cluster')
      stop()
    }


    #define label for later use in table
    period_label <- paste(as.Date(monitor_season[1]), '-',
                          as.Date(monitor_season[2]))

    # warn that times will be rounded to full day for entire monitoring period

    if(
        any( c(
        second(as.POSIXct(monitor_season[1])),
        minute(as.POSIXct(monitor_season[1])),
        hour(as.POSIXct(monitor_season[1])),

        second(as.POSIXct(monitor_season[2])),
        minute(as.POSIXct(monitor_season[2])),
        hour(as.POSIXct(monitor_season[2]))) > 0
    )
    ) {
      output <<- na.result('incorrect_date',
                           period_label)
      stop()
    }


    # make df of entire monitoring period (approx. a year)
    # recorded from given location
    data_year <- data %>%
      filter(home == home_num & location == location_type)
    

    if(metric %in% c('pm25', 'voc')) {
      # convert 0 values to LOD/2 in to allow for log-normal distrib. estimation
      data_year <- data_year %>%
        mutate_at(all_of(metric), function(x) ifelse(x==0, LOD/2, x))
    }


    # make column of all dates (increments of 5 min) within sampling year
    # including all locations
    all_dates_year <- seq.POSIXt(
      from = as.POSIXct(
        data %>%
      filter(home == home_num &
               location %in% c('living', 'bedroom', 'kitchen'))%>%
        pull(datetime) %>% min(), tz = 'UTC'),

      to =  as.POSIXct( data %>%
      filter(home == home_num &
               location %in% c('living', 'bedroom', 'kitchen'))%>%
        pull(datetime) %>% max(), tz = 'UTC'),
      by = '5 min')

        
    #calculate missingness of data in year for given location
    year_data_avail <-
      length(data_year %>% pull(datetime) %>% unique())/
      length(all_dates_year)




    # make df for recorded values in season for given location
    data_season <- data_year %>%
      # choose season date range
      filter(
        datetime >= ymd(monitor_season[1]) &
          datetime <= ymd(monitor_season[2])
      )


    # make column of all dates (increments of 5 min) within season
    all_dates_season <- seq.POSIXt(from = as.POSIXct( monitor_season[1], tz = 'UTC'),
                                   to =  floor_date(as.POSIXct( monitor_season[2],
                                                                tz = 'UTC')+24*60*60-1,
                                                    unit = '5 min'),
                                   by = '5 min')


    # calculate missingness of data points in the season
    season_data_avail <-
      length(data_season %>% pull(datetime) %>% unique())/
      length(all_dates_season)


    if(all_time == TRUE) {

      # stop if missing 25% of data in year
      if(year_data_avail < 0.75) {
        output <<- na.result(
          paste0('year_missing_',
                 signif((1-year_data_avail)*100, 2),
                 '%'),
          period_label
        )
        stop()
      }

      entire <- data_year

    } else {

          # stop if missing 25% of data in season
    if(season_data_avail < 0.75) {
      output <<- na.result(
        paste0('season_missing_',
               signif((1-season_data_avail)*100, 2),
               '%'),
        period_label
      )
      stop()
    }

      entire <- data_season
    }
    
    entire <- entire %>%
      group_by(day_time = hour(datetime)*3600 + minute(datetime)*60) %>%
      summarise_at(all_of(metric), list(mean = ~mean(., na.rm = TRUE)),
                   .groups = 'drop') %>%
      pull(mean)
    
    # ensure no na values
    if(any(is.na(entire))) {
      output <<- na.result('na_in_entire', period_label)
      stop()
    }
    


# stop if a 5_min period of day is completely missing data
if(length(entire) < 24*60/5) {
  output <<- na.result('missing_hour', period_label)
      stop()
}


    # function to take a running window of all short sampling periods possible
    # within the longer montirong period,

    kld.avg <- function(days) { # days = number of days in short samping period

      # for testing
      # y<- all_dates_season[(159*24*12):((159+28)*24*12)]

      a <-
        runner(
          all_dates_season,
          k = days*24*60/5, # number of 5-min periods in short sampling period
          # only evaluate windows that start at 12AM
          # and windows that are full (ignore partial windows at start)
          at = seq(days*24*60/5, length(all_dates_season), 24*60/5),
          f = function(y) { # y = window, vector (length = k) of specified days per iteration
            
            # filter out only given room
            # and the days specified by the days in window y
            sample <- data_season %>%
              filter( location == location_type & datetime %in% y)
            
            # omit sampling period if missing more than
            # 25 % of sampling period
            if(nrow(sample) < 0.75 * days*24*60/5) {
              
              a <- rep(NA, 24*60/5) %>% as.integer()
              
              
              
              # if not missing 25% of sampling period...
            } else {
              
             a <- sample %>%
               group_by(day_time = hour(datetime)*3600 + minute(datetime)*60) %>%
               summarise_at(all_of(metric), list(mean = ~mean(., na.rm = TRUE)),
                            .groups = 'drop') %>%
               pull(mean)
             
             # ensure no na values
             if(any(is.na(a))) {
               output <<- na.result('na_in_sample', period_label)
               stop()
             }
           
           # omit sampling period if there is not an avg value
           # for all 5-min period bins in the period
           if(length(a)<24*60/5) {
              a <- rep(NA, 24*60/5) %>% as.integer()

           }
            }  
            
            # return values for one running window
            a <- as.data.frame(a)
            colnames(a) <- y[1] # give name to column just to suppress "new name" message
            
            a
          }
        )
      
      # bind all elements of list into a dataframe
      a <- bind_cols(a)
      
      
      
      n_samples_possible <- ncol(a)
      # omit columns that have NA values (didn't have enough data)
      # use if statement to avoid changing
      # structure of single column to vector
      if(ncol(a)>1) a <- a[ , colSums(is.na(a)) == 0]
      
      # count number of sampling periods created (columns)
      n_samples <- ncol(a)
      
      # calculate proportion of samples that had sufficient data
      # for given short sampling frame length
      n_samp_avail <- n_samples/n_samples_possible
      

a <- lapply(colnames(a), function(x) {
  a %>%
  pull(x) %>%
    kld(entire)
})
  
# average all the resulting KLDs to find average KLD and standard error (se)
# for specified short sampling period length
list('kld_avg' = mean(unlist(a)),
     'kld_sd' = sd(unlist(a)),
     'n_samp_avail' = n_samp_avail)

    }


    # # test kld.avg function
    # test <- kld.avg(16)


    # calculate "max KLD":
    # average kld for short sampling periods
    # of minimum length, "sample_days_min"
    a <- kld.avg(sample_days_min)

    kld_avg_max <- a[['kld_avg']]

    kld_sd_max <- a[['kld_sd']]

    # apply function to find scaled_entropy dataframe for
    # sampling period of length "days2" to range of days in tested_sample_sequence
    entropy_data_list <- lapply(
      tested_sample_sequence,
      function (
        days2 # length of short sampling period
      ) {


        # calculate average KLD for all short sampling period of length "days2"
        a <- kld.avg(days2)
        kld_avg <- a[['kld_avg']]
        kld_sd <- a[['kld_sd']]



        # # calculate the sd for ratio of two means,
        # # assuming independence (no covariance) between sample of given size
        # # and minimum sample size
        # b <- (kld_avg/kld_avg_max)*sqrt(
        #   kld_sd^2/kld_avg^2+
        #     kld_sd_max^2/kld_avg_max^2
        # )


        a <- tibble(
          'coeff' = kld_avg/kld_avg_max, # mean scaled entropy value
          # # values for sd_int assuming kld_avg_min is a set number
          'high_val' = (kld_avg+kld_sd)/kld_avg_max,
          'low_val' = (kld_avg-kld_sd)/kld_avg_max,
          # # # values for sd_int assuming two independent samples
          # 'high_val2' = kld_avg/kld_avg_max+b,
          # 'low_val2' = kld_avg/kld_avg_max-b,
          'sample_length' = days2,
          'monitor_season' = period_label,
          'n_samp_avail' = a[['n_samp_avail']], # amount of samples with sufficient data
          'error' = 'none'
        )

        a

      }
    )


    # bind all into a dataframe
    a <- bind_rows(entropy_data_list)

    a # scaled_entropy for one date range

  } %>%
    # if season results in an error, return the error message in a df
    tryCatch(error = function(e) output)

  # find scaled_entropy for all short smpling lengths in all apecified time ranges

  a <- lapply(date_ranges_entire, scaled.entropy.season)


  # bind all into a dataframe
  entropy_data_season <- bind_rows(a) %>%
    mutate(method = 'time',
           home = home_num,
           metric = metric,
           location = location_type,
           period_compare = ifelse(all_time == TRUE, 'year', 'season'))


  entropy_data_season  # scaled_entropy for all specified date ranges

}




# test function--------------------------
start <- Sys.time()

test<- scaled.entropy.table.time('004', 'pm25',

                           date_ranges_entire = list(
                             c('2020-12-01', '2020-12-31'),
                                                     c('2020-11-01',
                                                       '2020-11-30')),
                           tested_sample_sequence = c(4,5,6),
                           location_type = 'living',
                           sample_days_min = 3,
                           all_time = TRUE)



end <- Sys.time()
run2 <- end- start

# # look into bootstrapping
# a_orig # all kld results
# boot1 <- sample(a_orig, size = 2, replace = T)
# boot <- lapply(1:20, function(i) sample(a_orig, replace = T))


```






```{r functions_time_structured}

# functions for time structure entropy calculations---------------------------

# function to calculate scaled entropy for a house
# during multiple different time periods
scaled.entropy.table <- function(home_num, metric,
months_entire = NULL, # months in entire monitoring period

# date range of entire monitoring period (if not month)
# in the form: list(c('YYYY-MM-DD', 'YYYY-MM-DD'),
# c('YYYY-MM-DD', 'YYYY-MM-DD'))
 date_ranges_entire = NULL,
tested_sample_sequence = c(1:28), # sequence of tested short sampling period lengths (in days)
sample_days_min = 1, # amount of days in shortest theoretical sample
room_type = 'living',
data = rtdata) {
  
  
# function to find scaled_entropy for samples in a given monitor month
  
scaled.entropy.month <- function(month_monitor) {
# if date range is specified, month monitor will be a date range
  # if month is specified, month_monitor will be a month
  
  # check to month and date range are not BOTH specified
  # for entire monitoring period
   if((!is.null(date_ranges_entire) &
       !is.null(months_entire))) stop(paste0('Specify either month ',
      'or date range of entire\n monitoring period. ',
      'Do not specify both.'))
  
# if date range is specified for entire monitoring period
  if(is.null(months_entire)){ 

    # must unlist the listed range that is used in lapply funnction
    month_monitor <- unlist(month_monitor)
    
        #define terms for later use
    monitor_period <- paste(as.Date(month_monitor[1]), '-',
                             as.Date(month_monitor[2]))

    # warn that times will be rounded to full day for entire monitoring period
    if(second(as.POSIXct(month_monitor[1])) != 0 |
       minute(as.POSIXct(month_monitor[1])) != 0 |
       hour(as.POSIXct(month_monitor[1])) != 0|
       
       second(as.POSIXct(month_monitor[2])) != 0 |
       minute(as.POSIXct(month_monitor[2])) != 0 |
       hour(as.POSIXct(month_monitor[2])) != 0) warning('Times specifed in date range endpoints are not acknowledged.')

    
 # make column of specified metric for one home, one room
all_data <- data %>%
  filter(home == home_num, room == room_type) %>%
  dplyr::select(c(date, all_of(metric)))%>%
  # choose long-term monitoring date range
  filter(
    date >= floor_date(as.POSIXct(month_monitor[1], format="%Y-%m-%d",
                              na = "NA", tz = 'UTC'), unit = 'day'),
    date < ceiling_date(as.POSIXct(month_monitor[2], format="%Y-%m-%d",
                              na = "NA", tz = 'UTC')+1, unit = 'day')
  ) %>%
  # make columns for hour of day, day of week, and month of year
  mutate(hour = hour(date), day = weekdays(date), month = month(date),
         # make column for day (so they can be grouped later)
         day_date = floor_date(date, unit = 'day'))
  }
  


 # but if month is specified for entire monitoring period
  if(!is.null(months_entire)){ 
    
        #define terms for later use
    monitor_period <- month_monitor
    
    
 # make column of specified metric for one home, one room
all_data <- data %>%
  filter(home == home_num, room == room_type) %>%
  dplyr::select(c(date, all_of(metric)))%>%
  # make a month column
         mutate(entire_period = month(date, label = TRUE)) %>%
    filter(entire_period == month_monitor) %>% # filter by specified month
  # make coluns for hour of day, day of week, and month of year
  mutate(hour = hour(date), day = weekdays(date), month = month(date),
         # make column for day (so they can be grouped later)
         day_date = floor_date(date, unit = 'day'))
  } 
  
  
  
# make a vector of days in all_data
day_col <- all_data$day_date %>%
  unique() 


# make column of all dates within range

if(is.null(months_entire)) {# if using date range...
  
  all_dates <- seq.POSIXt(from = floor_date(
    as.POSIXct( month_monitor[1],
                tz = 'UTC'), unit = 'day'),
                    to = floor_date(
                      as.POSIXct( month_monitor[2],
                                  tz = 'UTC'), unit = 'day'),
                    by = 60*60*24)
  
} else { # or if using month
 
  # assume only July 2020 - June 2021 were sampled
  if(match(month_monitor, month.abb) %in% 7:12) year_sample <- '2020'
  if(match(month_monitor, month.abb) %in% 1:6) year_sample <- '2021'

     all_dates <- seq.POSIXt(from = floor_date(
    as.POSIXct( paste0(year_sample, '-',
                       match(month_monitor, month.abb),
                      '-15'),
                tz = 'UTC'), unit = 'month'),
                    to = ceiling_date(
    as.POSIXct( paste0(year_sample, '-',
                       match(month_monitor, month.abb),
                      '-15'),
                tz = 'UTC'), unit = 'month')-ddays(1),
                    by = 60*60*24)

}


# count missing days in the entire data set


day_miss <- length(all_dates) - length(day_col)
         
if(day_miss > 0) warning(paste('missing ', day_miss, 'day(s) in long monitoring period'))


# find average hour-of-day measurements for entire monitoring period
# throw error if there is not enough data for 24 hour-of-day averages
entire <- all_data %>%
  group_by(hour) %>%
  summarise_at(all_of(metric), list(mean = ~mean(., na.rm = TRUE),
                                    # count amount of 15 min values
                                    # that were averaged for each
                                    # hour-of-day avg value
                                    n_15min = ~n())) %>%
  ungroup()

if(nrow(entire) < 24) stop(paste0('do not have enough data to create average value for all 24 hours of day in ', monitor_period))



# function to take a running window of all short sampling periods possible
# within the longer montirong period,
# find the hour-of-day averages for each short sampling period,
# and return a dataframe with the results for each sampling period
# in its own column 

# NOTES:
## will stop if there are less than 24 hour-of-day averages in a given
         # short sampling period

kld.avg <- function(days) { # days = number of days in short samping period
  
 a <- 
    runner(all_dates,
       k = days, # number of days in short sampling period
       at = c(days:length(all_dates)),
       f = function(y) { # y = vector of specified days (length = k) of day_col per iteration
         
         a <- all_data %>%
           # filter out only the amount of days specified by k
           # from the all_dates col
           filter(day_date %in% y) 
         
         n_data <- nrow(a)
         # omit sampling period if missing more than
         # 25 % of sampling period
         if(n_data < 0.75 * 4 * 24 * days) { 
           
           a <- tibble('missing' = as.numeric(matrix(NA, nrow = 24)))

         
           # if not missing 25% of sampling period...
         } else { 
           
           a <- a %>%
             # take by-hour mean values
             group_by(hour) %>%
             summarise_at(all_of(metric), mean, na.rm = TRUE) %>%
             ungroup() %>%
             #result in only the metric averages
             dplyr::select(-hour)
           

           n_avg <- nrow(a)
           
           # omit sampling period if there is not an avg value
           # for all 24 hour-of-day slots in the period
           if(n_avg<24) {
             a <- tibble('missing' = as.numeric(
               matrix(NA, nrow = 24)))
           }
         }
         
         # return values for all running windows
         a

       }
  )

  

# bind all elements of list into a dataframe
a <- do.call(cbind, a) %>%
  data.frame() 

n_samples_possible <- ncol(a)
# omit columns that have NA values (didn't have enough data)
# use if statement to avoid changing
# structure of single column to vector
if(ncol(a)>1) a <- a[ , colSums(is.na(a)) == 0]

a <-as.data.frame(a)

# count number of sampling periods created (columns)
n_samples <- ncol(a)


# calculate proportion of samples that had sufficient data
# for given short sampling frame length
n_samp_avail <- n_samples/n_samples_possible


# apply KLD between the each short sample period and
# the entire monitroing period individually

a <- sapply(colnames(a), function(x) {
  a %>%
  pull(x) %>%
    entropy::KL.plugin(pull(entire, mean), unit = 'log2')
})
  
# average all the resulting KLDs to find average KLD and standard error (se)
# for specified short sampling period length
list('kld_avg' = mean(a),
     'kld_se' = sd(a)/length(a),
     'n_samp_avail' = n_samp_avail)

}


# test kld.avg function
  # test <- kld.avg(20)

# calculate "max average KLD" and its se value:
# average kld for short sampling periods of length of short sampling period
# of minimum length, "sample_days_min"
a <- kld.avg(sample_days_min)
kld_avg_max <- a[['kld_avg']]
kld_se_max <- a[['kld_se']]


# apply function to find scaled_entropy dataframe for
# sampling period of length "sample_days" to range of 1:28 days

entropy_data_list <- lapply(tested_sample_sequence,
                        function (
                          days2 # length of short sampling period
                                  ) {

                          
                          
# calculate average KLD and se for all short sampling period of length "days2"
a <- kld.avg(days2)


# calculate the confidence interval for ratio of two means
# https://i.stack.imgur.com/vO8Ip.png
b <- ((a[['kld_avg']])/kld_avg_max)*
  sqrt(
    ((a[['kld_se']])^2)/((a[['kld_avg']])^2)+ (kld_se_max^2)/(kld_avg_max^2))



a <- tibble(
  'coeff' = a[['kld_avg']]/kld_avg_max, # mean scaled entropy value
  # values for confidence intervals
  'high_val' = a[['kld_avg']]/kld_avg_max+1.96*b,
    'low_val' = a[['kld_avg']]/kld_avg_max-1.96*b,
     'home' = home_num,
  'metric' = metric,
     'sample_length' = days2,
  'monitor_period' = monitor_period,
  'n_samp_avail' = a[['n_samp_avail']] # amount of samples with sufficient data
  )


a

                        }
)


# bind all into a dataframe
entropy_data <- do.call(rbind, entropy_data_list)

entropy_data # scaled_entropy for one month

}

# test function for one month
# a <- scaled.entropy.month(month_monitor = 'Dec')




# find scaled_entropy for all short smpling lengths in all apecified months
# or time periods

if(is.null(months_entire)) {# if using date range...

entropy_data_month_list <- lapply(date_ranges_entire, scaled.entropy.month)

}else{# if using months...
  entropy_data_month_list <- lapply(months_entire, scaled.entropy.month)

}

# bind all into a dataframe
entropy_data_month <- do.call(rbind, entropy_data_month_list) %>%
  mutate(method = 'time')


entropy_data_month  # scaled_entropy for all specified months

}

# test function
scaled.entropy.table('008', 'pm25', c('Dec', 'Nov'),
                     tested_sample_sequence = c(4,5,6), sample_days_min = 3)
####################

# functions to make data -----------------------------

# make dataframes and name in form: "home_009_pm25"
# so they can be saved separately and then joined
make.data <- function(hm, metric, month,
                      tested_sample_sequence) {
  tryCatch(scaled.entropy.table(hm, metric,
                                months_entire = month,
                                tested_sample_sequence = tested_sample_sequence),
         error = function(e) NULL)
  
}

rep.data <- function (hm, metric, mnths,
                      tested_sample_sequence = c(1:28)) {
  
  a <- lapply(
  # loop through for homes
  hm,
               # loop through for months
               function(hm2) {
                 lapply(month.abb[mnths],
                      make.data,
                      hm = hm2,
                      metric = metric,
                      tested_sample_sequence = tested_sample_sequence)
                 })
  # remove one level of lists
  b <- flatten(a)
# combine all lists into a dataframe
c <- do.call(rbind, b)

assign(paste('home', hm, metric, sep = '_'),
         c,
         envir = .GlobalEnv)

}

# making data for date range period------------------------------------------------

# make dataframes and name in form: "home_009_pm25"
# so they can be saved separately and then joined
make.data.range <- function(hm, metric, range,
                      tested_sample_sequence) {
  tryCatch(scaled.entropy.table(hm, metric,
                                date_ranges_entire = range,
                                tested_sample_sequence = tested_sample_sequence),
         error = function(e) NULL)
  
}



rep.data.range <- function (hm, metric, ranges,
                      tested_sample_sequence = c(1:28)) {
  
  a <- lapply(
  # loop through for homes
  hm,
               # loop through for months
               function(hm2) {
                 lapply(ranges,
                      make.data.range,
                      hm = hm2,
                      metric = metric,
                      tested_sample_sequence = tested_sample_sequence)
                 })
  # remove one level of lists
  b <- flatten(a)
# combine all lists into a dataframe
c <- do.call(rbind, b)

assign(paste('home', hm, metric, sep = '_'),
         c,
         envir = .GlobalEnv)

}



 # name files that were created
file.name <- function (hm, metric) {
  paste('home', hm, metric, sep = '_')
}



```

```{r define_variables}


# list of all home numbers
homes_all <- home.list(1:17)

# sequence of representative thresholds to test
threshold_seq <- c(0.1, 0.2, 0.3)


```



## Time structure


```{r time_data_maker_range, eval = FALSE}



# pm25--------------------------------------

met <- 'pm25'

home1 <- make.data.range('001', met, season_list, sequence)
home2 <- make.data.range('002', met, season_list, sequence)
home3 <- make.data.range('003', met, season_list, sequence)
home4 <- make.data.range('004', met, season_list, sequence)
home5 <- make.data.range('004', met, season_list, sequence)
home6 <- make.data.range('006', met, season_list, sequence)
home7 <- make.data.range('007', met, season_list, sequence)
home8 <- make.data.range('008', met, season_list, sequence)
home9 <- make.data.range('009', met, season_list, sequence)
home10 <- make.data.range('010', met, season_list, sequence)
home11 <- make.data.range('011', met, season_list, sequence)
home12 <- make.data.range('012', met, season_list, sequence)
home13 <- make.data.range('013', met, season_list, sequence)
home14 <- make.data.range('014', met, season_list, sequence)
home15 <- make.data.range('015', met, season_list, sequence)
home16 <- make.data.range('016', met, season_list, sequence)


# make a list of all dataframes
home_data_list <- lapply(c(1:16),
                        function(x) tryCatch(get(paste0('home', x)),
                                             error = function(e){
                                               NULL
                                               }
                                             ))


#combine into one dataframe
home_data <- do.call(rbind, home_data_list)


# to combine new data with old data
# home_data <- do.call(home_data, entropy_season_pm25)


# make csv of all season data
write_csv(home_data, file =
            paste0('./representativeness_data/rep_data_season_', met, '_', Sys.Date(), '.csv'))



# voc--------------------------------------

met <- 'voc'

home1 <- make.data.range('001', met, season_list, sequence)
home2 <- make.data.range('002', met, season_list, sequence)
home3 <- make.data.range('003', met, season_list, sequence)
home4 <- make.data.range('004', met, season_list, sequence)
home5 <- make.data.range('004', met, season_list, sequence)
home6 <- make.data.range('006', met, season_list, sequence)
home7 <- make.data.range('007', met, season_list, sequence)
home8 <- make.data.range('008', met, season_list, sequence)
home9 <- make.data.range('009', met, season_list, sequence)
home10 <- make.data.range('010', met, season_list, sequence)
home11 <- make.data.range('011', met, season_list, sequence)
home12 <- make.data.range('012', met, season_list, sequence)
home13 <- make.data.range('013', met, season_list, sequence)
home14 <- make.data.range('014', met, season_list, sequence)
home15 <- make.data.range('015', met, season_list, sequence)
home16 <- make.data.range('016', met, season_list, sequence)

# make a list of all dataframes
home_data_list <- lapply(c(1:16),
                        function(x) tryCatch(get(paste0('home', x)),
                                             error = function(e){
                                               NULL
                                               }
                                             ))


#combine into one dataframe
home_data <- do.call(rbind, home_data_list)

# to combine new data with old data
# home_data <- rbind(home_data, entropy_season_voc)


# make csv of all season data
write_csv(home_data, file =
            paste0('./representativeness_data/rep_data_season_', met, '_', Sys.Date(), '.csv'))




# temp--------------------------------------

met <- 'temp'

home1 <- make.data.range('001', met, season_list, sequence)
home2 <- make.data.range('002', met, season_list, sequence)
home3 <- make.data.range('003', met, season_list, sequence)
home4 <- make.data.range('004', met, season_list, sequence)
home5 <- make.data.range('004', met, season_list, sequence)
home6 <- make.data.range('006', met, season_list, sequence)
home7 <- make.data.range('007', met, season_list, sequence)
home8 <- make.data.range('008', met, season_list, sequence)
home9 <- make.data.range('009', met, season_list, sequence)
home10 <- make.data.range('010', met, season_list, sequence)
home11 <- make.data.range('011', met, season_list, sequence)
home12 <- make.data.range('012', met, season_list, sequence)
home13 <- make.data.range('013', met, season_list, sequence)
home14 <- make.data.range('014', met, season_list, sequence)
home15 <- make.data.range('015', met, season_list, sequence)
home16 <- make.data.range('016', met, season_list, sequence)

# make a list of all dataframes
home_data_list <- lapply(c(1:16),
                        function(x) tryCatch(get(paste0('home', x)),
                                             error = function(e){
                                               NULL
                                               }
                                             ))

#combine into one dataframe
home_data <- do.call(rbind, home_data_list)

# to combine new data with old data
# home_data <- rbind(home_data, entropy_season_temp)


write_csv(home_data, file =
            paste0('./representativeness_data/rep_data_season_', met, '_', Sys.Date(), '.csv'))




# noise--------------------------------------

met <- 'noise'


home1 <- make.data.range('001', met, season_list, sequence)
home2 <- make.data.range('002', met, season_list, sequence)
home3 <- make.data.range('003', met, season_list, sequence)
home4 <- make.data.range('004', met, season_list, sequence)
home5 <- make.data.range('004', met, season_list, sequence)
home6 <- make.data.range('006', met, season_list, sequence)
home7 <- make.data.range('007', met, season_list, sequence)
home8 <- make.data.range('008', met, season_list, sequence)
home9 <- make.data.range('009', met, season_list, sequence)
home10 <- make.data.range('010', met, season_list, sequence)
home11 <- make.data.range('011', met, season_list, sequence)
home12 <- make.data.range('012', met, season_list, sequence)
home13 <- make.data.range('013', met, season_list, sequence)
home14 <- make.data.range('014', met, season_list, sequence)
home15 <- make.data.range('015', met, season_list, sequence)
home16 <- make.data.range('016', met, season_list, sequence)


# make a list of all dataframes
home_data_list <- lapply(c(1:16),
                        function(x) tryCatch(get(paste0('home', x)),
                                             error = function(e){
                                               NULL
                                               }
                                             ))

#combine into one dataframe
home_data <- do.call(rbind, home_data_list)


# to combine new data with old data
# home_data <- rbind(home_data, entropy_season_noise)


write_csv(home_data, file =
            paste0('./representativeness_data/rep_data_season_', met, '_', Sys.Date(), '.csv'))




# light--------------------------------------

met <- 'light'



home1 <- make.data.range('001', met, season_list, sequence)
home2 <- make.data.range('002', met, season_list, sequence)
home3 <- make.data.range('003', met, season_list, sequence)
home4 <- make.data.range('004', met, season_list, sequence)
home5 <- make.data.range('004', met, season_list, sequence)
home6 <- make.data.range('006', met, season_list, sequence)
home7 <- make.data.range('007', met, season_list, sequence)
home8 <- make.data.range('008', met, season_list, sequence)
home9 <- make.data.range('009', met, season_list, sequence)
home10 <- make.data.range('010', met, season_list, sequence)
home11 <- make.data.range('011', met, season_list, sequence)
home12 <- make.data.range('012', met, season_list, sequence)
home13 <- make.data.range('013', met, season_list, sequence)
home14 <- make.data.range('014', met, season_list, sequence)
home15 <- make.data.range('015', met, season_list, sequence)
home16 <- make.data.range('016', met, season_list, sequence)


# make a list of all dataframes
home_data_list <- lapply(c(1:16),
                        function(x) tryCatch(get(paste0('home', x)),
                                             error = function(e){
                                               NULL
                                               }
                                             ))

#combine into one dataframe
home_data <- do.call(rbind, home_data_list)


# to combine new data with old data
# home_data <- rbind(home_data, entropy_season_light)


write_csv(home_data, file =
            paste0('./representativeness_data/rep_data_season_', met, '_', Sys.Date(), '.csv'))



# all_variables------------------------------------------------


entropy_all <- rbind(entropy_pm25, entropy_voc,
                     entropy_temp, entropy_noise) %>%
  # make a column specifying shoulder seasons
  mutate(
    season = case_when(
      monitor_period %in% month.abb[c(9:11, 3:5)] ~ 'shoulder',
          monitor_period %in% month.abb[c(12,1:2, 6:8)] ~ 'non-shoulder'
    )
  )



# for one home:
a <- entropy_all %>%
  filter(home == '006' & metric == 'pm25_ihs')

# plot results
ggplot(aes(x = sample_length, y = coeff,
           color = season, shape = monitor_period), data = a)+
  geom_line()+
  geom_point(size = 1)+
  xlab('Sample Length, days')+
  ylab('Scaled Relative Entropy')+
  geom_hline(yintercept=0.1, linetype="dashed", 
                color = "black", size=0.5)+
  coord_cartesian(ylim = c(0,0.5))


# further testing---------------------------------------------------------
# ############ voc
# met <- 'voc_ihs'
# 
# # home_003
# a <- scaled.entropy.table('003', met, months_entire = c('Dec', 'Jan'))
# 
b <- scaled.entropy.table('003', met, months_entire = c('Aug'))
# c <- scaled.entropy.table('003', met, months_entire = c('Sep'))
# 
# 
# # home_006
# d <- scaled.entropy.table('006', met, months_entire = c('Sep'))
# 
# e <- scaled.entropy.table('006', met, months_entire = c('Nov'))
# f <- scaled.entropy.table('006', met, months_entire = c('Dec'))
# g <- scaled.entropy.table('006', met, months_entire = c('Jan'))
# 
# # bind all together
# all <- rbind(a,b,c,d,e,f,g)
# 
# # make csv file
# write_csv(all, path = paste0('scaled_entropy_months_', met,
#                              '_', Sys.Date(),'.csv'))
# 
# # read in csv file
# entropy_voc <- read_csv(file = paste0('scaled_entropy_months_', met, '.csv'))






######################### join all metrics and plot

entropy_all <- full_join(entropy_pm25, entropy_voc) %>%
  full_join(entropy_temp)

write_csv(entropy_all, path = paste0('rep_data_all_',
                             Sys.Date(),'.csv'))

# home_003
a <- entropy_all %>%
  filter(home == '003')

# change order for plotting
a$metric = factor(a$metric, levels=c('pm25_ihs','voc_ihs','temp'))

# plot results
ggplot(aes(x = sample_length, y = coeff, color = monitor_period), data = a)+
  geom_line()+
  facet_wrap('metric')+
  xlab('Sample Length, days')+
  ylab('Scaled Relative Entropy')+
  geom_hline(yintercept=0.1, linetype="dashed", 
                color = "black", size=0.5)+
  coord_cartesian(ylim = c(0,0.5))



# home_006
a <- entropy_all %>%
  filter(home == '006')

# change order for plotting
a$metric = factor(a$metric, levels=c('pm25_ihs','voc_ihs','temp'))

# plot results
ggplot(aes(x = sample_length, y = coeff, color = monitor_period), data = a)+
  geom_line()+
  facet_wrap('metric')+
  xlab('Sample Length, days')+
  ylab('Scaled Relative Entropy')+
  geom_hline(yintercept=0.1, linetype="dashed", 
                color = "black", size=0.5)+
  coord_cartesian(ylim = c(0,0.5))






```



```{r time_plotter}


# import entropy data
entropy_pm25 <- read_csv(file = './representativeness_data/rep_data_pm25.csv')
entropy_voc <- read_csv(file = './representativeness_data/rep_data_voc.csv')
entropy_temp <- read_csv(file = './representativeness_data/rep_data_temp.csv')
entropy_noise <- read_csv(file = './representativeness_data/rep_data_noise.csv')


entropy_season_pm25 <- read_csv(
  file = './representativeness_data/rep_data_season_pm25.csv')
entropy_season_voc <- read_csv(
  file = './representativeness_data/rep_data_season_voc.csv')
entropy_season_temp <- read_csv(
  file = './representativeness_data/rep_data_season_temp.csv')
entropy_season_noise <- read_csv(
  file = './representativeness_data/rep_data_season_noise.csv')


entropy_all <- rbind(entropy_pm25, entropy_voc,
                     entropy_temp, entropy_noise,
                     entropy_season_pm25, entropy_season_voc,
                     entropy_season_temp, entropy_season_noise) %>%
  mutate(
      # make a column specifying shoulder seasons
    season = case_when(
      monitor_period %in% month.abb[c(10, 4)] ~ 'shoulder',
          monitor_period %in% month.abb[c(11:12,1:3, 5:9)] ~ 'non-shoulder'),
    # convert time periods to seasons
    monitor_period = case_when(
      monitor_period == paste(as.POSIXct(fall[1]),
                              as.POSIXct(fall[2]), sep = ' - ') ~ 'fall',
            monitor_period == paste(as.POSIXct(winter[1]),
                              as.POSIXct(winter[2]), sep = ' - ') ~ 'winter',
                  monitor_period == paste(as.POSIXct(decemb[1]),
                              as.POSIXct(decemb[2]), sep = ' - ') ~ 'decemb',
      TRUE ~ monitor_period

    )
  )




entropy.plot.metrics <- function (data, hm, metrics,
                                 # omit a sampling period of given length
                                 # if this proportion of possible samples
                                 # were not availabel in data
                                 sample_omit = 0.5
                                 ) {
  
# for one home:
a <- data %>%
  filter(home == hm &
           metric %in% metrics &
           n_samp_avail >= sample_omit)

# plot results
ggplot(aes(x = sample_length, y = coeff,
           color = season, shape = monitor_period), data = a)+
  geom_line()+
  geom_point(size = 1)+
  xlab('Sample Length, days')+
  ylab('Scaled Relative Entropy')+
  geom_hline(yintercept=0.1, linetype="dashed", 
                color = "black", size=0.5)+
  coord_cartesian(ylim = c(0,0.5))+
  ggtitle(paste('Home',hm)) +
  facet_wrap('metric') +
  scale_x_continuous(breaks=seq(0,28,7))+
  geom_ribbon(aes(ymin = low_val, ymax = high_val, fill = season),
              alpha = 0.3, color = NA)
}

# # test for one home
# entropy.plot.months('006',c('pm25_ihs', 'voc_ihs', 'temp', 'noise'))

a <- entropy_all %>%
  filter(
    monitor_period != 'fall' &
      monitor_period != 'winter' &
      monitor_period != 'decemb'
           )


# make plots for all homes
lapply(home.list(c(1,3,4,6:16)),
       entropy.plot.metrics,
              data = a,
       metrics = c('pm25', 'voc', 'temp', 'noise'))





entropy.plot.homes <- function (data, hm, metrics,
                                 # omit a sampling period of given length
                                 # if this proportion of possible samples
                                 # were not availabel in data
                                 sample_omit = 0.5
                                 ) {
  
# for one home:
a <- data %>%
  filter(metric == metrics &
           home %in% hm &
           n_samp_avail >= sample_omit)

# plot results
ggplot(aes(x = sample_length, y = coeff,
           color = season, shape = monitor_period), data = a)+
  geom_line()+
  geom_point(size = 1)+
  xlab('Sample Length, days')+
  ylab('Scaled Relative Entropy')+
  geom_hline(yintercept=0.1, linetype="dashed", 
                color = "black", size=0.1)+
  coord_cartesian(ylim = c(0,0.5))+
  ggtitle(paste(metrics)) +
  facet_wrap('home') +
  scale_x_continuous(breaks=seq(0,28,7))+
  geom_ribbon(aes(ymin = low_val, ymax = high_val, fill = season),
              alpha = 0.3, color = NA)
}

# # plot all homes on one faceted plot
# a <- entropy_all %>%
#   filter(
#     monitor_period != 'fall' &
#       monitor_period != 'winter' &
#       monitor_period != 'decemb'
#            )
# 
# 
# # make plots for all metrics
# lapply(c('pm25_ihs', 'voc_ihs', 'temp', 'noise'),
#        entropy.plot.homes,
#        data = a,
#        hm = home.list(c(1,3,4,6:16)))




entropy.plot.metrics.seasons <- function (data, hm, metrics,
                                 # omit a sampling period of given length
                                 # if this proportion of possible samples
                                 # were not availabel in data
                                 sample_omit = 0.5
                                 ) {
  
# for one home:
a <- data %>%
  filter(home == hm &
           metric %in% metrics &
           n_samp_avail >= sample_omit)

# plot results
ggplot(aes(x = sample_length, y = coeff,
           color = monitor_period), data = a)+
  geom_line()+
  geom_point(size = 1)+
  xlab('Sample Length, days')+
  ylab('Scaled Relative Entropy')+
  geom_hline(yintercept=0.1, linetype="dashed", 
                color = "black", size=0.5)+
  coord_cartesian(ylim = c(0,0.5))+
  ggtitle(paste('Home',hm)) +
  facet_wrap('metric') +
  scale_x_continuous(breaks=seq(0,28,7))+
  geom_ribbon(aes(ymin = low_val, ymax = high_val, fill = monitor_period),
              alpha = 0.3, color = NA)
}

a <- entropy_all %>%
  filter(
    monitor_period == 'fall' |
      monitor_period == 'winter' |
      monitor_period == 'decemb'
           )

# make separate plots for all homes
lapply(home.list(c(3,4,6:14)),
       entropy.plot.metrics.seasons,
              data = a,
       metrics = c('pm25', 'voc', 'temp', 'noise'))




```


### Effect on when average Scaled Entropy crosses below threshold


```{r threshold_plots_time, eval = FALSE}

# months--------------------------------------------------------------

# make dataframe of values for minimum sample length (in days) where entropy is less than 0.1
entropy_cutoffs <- entropy_all %>%
  filter(n_samp_avail >0.5) %>%
  filter(
    monitor_period %in% month.abb
    ) %>%
  filter(coeff <= thershold) %>% 
  group_by(home, metric, monitor_period) %>%
  summarise_at(vars(sample_length), min) %>%
  ungroup()

# fill in missing combinations so bar chart will have bars of constant width
a <- expand.grid(home=unique(entropy_cutoffs$home),
                 metric=unique(entropy_cutoffs$metric),
                 monitor_period=unique(entropy_cutoffs$monitor_period))%>%
  data.frame() %>%
  left_join(entropy_cutoffs)



# bar chart of when threshold was reached for each home
ggplot(aes(x = home, y = sample_length, fill = metric), data = a)+
  geom_col(position = 'dodge')+
  facet_wrap(vars(monitor_period), nrow = 2)







# make dataframe of values for minimum sample length (in days)
# where entropy is less than threshold value
threshold.freq.table <- function (threshold) {
  
  
  entropy_cutoffs <- entropy_all %>%
  filter(n_samp_avail >0.5) %>%
  filter(
    monitor_period %in% month.abb
    ) %>%
  filter(coeff <= threshold) %>% 
  group_by(home, metric, monitor_period) %>%
  summarise_at(vars(sample_length), min) %>%
  ungroup()
    

  # fill in missing combinations so bar chart will have bars of constant width
a <- expand.grid(home=unique(entropy_cutoffs$home),
                 metric=unique(entropy_cutoffs$metric),
                 monitor_period=unique(entropy_cutoffs$monitor_period))%>%
  data.frame() %>%
  left_join(entropy_cutoffs) %>%
# count frequency of home-month instances when threshold was reached for each sample length
  group_by(sample_length, metric) %>%
  summarise(freq = n()) %>%
  ungroup() %>%
  filter(!is.na(sample_length)) %>%
  mutate(thresh = as.factor(threshold))
}


# apply function to all thresholds in threshold_seq
 a <- lapply(threshold_seq, threshold.freq.table.seasons)

 a <- do.call(rbind, a)

 # ensure there weren't a significant amount of omissions for any metric type
 # (because the threshod may never have been reached)
 
 # test <- a %>%
 #   group_by(metric) %>%
 #   summarise(n = sum(freq)) %>%
 #   ungroup()

 # histogram of when threshold was reached for all home/month combos
ggplot(aes(x = sample_length, fill = thresh), data = a)+
  geom_density(alpha = 0.5)+
  facet_wrap(vars(metric))+
  xlab("Sample Length Required to Meet Representativeness Threshold, days")+
  scale_x_continuous(breaks = c(7,14,21,28))


```



---
title: "Representativeness"
output:
  html_document:
    toc: true
    toc_float: true
    theme: paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.path = 'figures/',
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  fig.width = 10, fig.height = 3,
  cache = FALSE)
```

```{r libraries, include=FALSE}
library(dplyr)
library(purrr)
library(tidyr)
library(openair)
library(ggplot2)
library(readr)
library(googlesheets4)
library(lubridate)
library(gridExtra)
library(runner) # for moving window functions
library(ggpubr) # for tesing log-normality
# library(cubature) # for alternative method of integrating kld function
# library(entropy) # for KLD function
# library(fitdistrplus) # for fitting distribution to data NOTE, MASS::select CONFLICTS WITH dplyr::select

```


```{r import_omni_minute_data}
omni_data<- read_csv('./csv_created/omni_all_locations.csv')


zero_test <- omni_data %>%
  group_by(home, location) %>%
  summarize(pm25_zeros = sum(pm25 == 0), pm25_n = n()) %>%
  mutate(pm25_zero_pct = pm25_zeros/pm25_n*100) %>%
  ungroup()

```



```{r functions_misc}

#Function to only display 3 significant figures (for tables)
signif3 <- function(x){
  signif(x, digits = 3)
}

##Define a char vector of home numbers using a number vector,
##ex: x <- home.list(c(1:15)) for homes 1-15
threedig <- function(x) {
  if (nchar(x) == 1) {a <- paste0('00', x)
  return(a)
  }
  
  if (nchar(x) == 2) {a<- paste0('0', x)
  return(a)
  }
  else return(a)
}

home.list <- function(x) sapply(x, threedig)


#function to make density plot of variable x (a column)
dens.plot <- function(data, metric, rm) {
  
ggdensity(data %>%
            filter(room == rm) %>%
            pull(all_of(metric)), 
          main = paste("Density plot of", rm, metric),
          xlab = metric)
}




```


```{r variables}

# list of all home numbers
homes_all <- home.list(1:17)

# thresholds for representativeness
rep_threshold_seq <- c(0.4, 0.5, 0.6)

```




## Distribution Shape: Assume Log-normal distribution

```{r function_shape_master}


# give a row of NA values with identifers to help with identifying
# causes of errors in function
na.result <- function(error, season = NA, sample_days = NA, samp_avail = NA) {
  tibble(
  'coeff' = NA,
  'high_val' = NA,
  'low_val' = NA,
  'sample_length' = sample_days, # sample_length,
  'monitor_season' = season, # period_label,
  'n_samp_avail' = samp_avail, # sample_available, # amount of samples with sufficient data
  'error' = error
)
}


# calculate kld for continuous probability distributions

# define variables for testing-----------------------
date_ranges_entire <- list(c('2020-11-01','2020-11-30'))
tested_sample_sequence <- seq(3,28,4)
sample_days_min <- 3
home_num <- '004'
location_type <- 'living'
data <- omni_data
monitor_season <- date_ranges_entire[1]
metric <- 'pm25'
days <- 16
days2 <- 16
all_time <- TRUE

# remove variables after done testing
rm(tested_sample_sequence,
   date_ranges_entire,
sample_days_min,
home_num,
monitor_season,
monitor_period,
metric,
days,
days2,
all_time,
data,
data_season,
data_year,
entire,
date_col_season,
date_col_year,
all_dates_season,
all_dates_year
)



# entropy function minutely data ----------------------------




# function to calculate scaled entropy for a house
# during multiple different time periods
scaled.entropy.table.shape <- function(
  home_num, metric,
  
  # date range of entire monitoring period
  # in the form: list(
  #                    c('YYYY-MM-DD', 'YYYY-MM-DD'),
  #                    c('YYYY-MM-DD', 'YYYY-MM-DD')
  #                  )
  # data in start and end day included
  date_ranges_entire,
  tested_sample_sequence, # sequence of tested short sampling period lengths (in days)
  location_type, # sensor location (living, bedroom, or kitchen)
  all_time = FALSE, # if TRUE, compare each short sampling period to entire monitoring year of home
  sample_days_min, # amount of days in shortest theoretical sample
  
  data = omni_data) {
  
  output <<- na.result(NA) # clear error output in case it was triggered previously
  
  if(metric %in% c('pm25', 'voc')) {
    # define LOD of metric to be 1/2 minimum detected (non-zero) value
    LOD <- data %>%
      filter(!!sym(metric)!=0) %>%
      pull(all_of(metric))%>%
      min()
  }
  

  # function to find scaled_entropy for samples in a given monitor season

  scaled.entropy.season <- function(monitor_season) {


    # must unlist the listed range that is used in lapply funnction
    monitor_season <- unlist(monitor_season)

    # stop if monitoring season is empty
    if(is_empty(monitor_season)) {
      output <<- na.result('likely_no_cluster')
      stop()
    }


    #define label for later use in table
    period_label <- paste(as.Date(monitor_season[1]), '-',
                          as.Date(monitor_season[2]))

    # warn that times will be rounded to full day for entire monitoring period

    if(
        any( c(
        second(as.POSIXct(monitor_season[1])),
        minute(as.POSIXct(monitor_season[1])),
        hour(as.POSIXct(monitor_season[1])),

        second(as.POSIXct(monitor_season[2])),
        minute(as.POSIXct(monitor_season[2])),
        hour(as.POSIXct(monitor_season[2]))) > 0
    )
    ) {
      output <<- na.result('incorrect_date',
                           period_label)
      stop()
    }


    # make df of entire monitoring period (approx. a year)
    # recorded from given location
    data_year <- data %>%
      filter(home == home_num & location == location_type)
    

    if(metric %in% c('pm25', 'voc')) {
      # convert 0 values to LOD/2 in to allow for log-normal distrib. estimation
      data_year <- data_year %>%
        mutate_at(all_of(metric), function(x) ifelse(x==0, LOD/2, x))
    }


    # make column of all dates (increments of 5 min) within year
    # including all locations
    all_dates_year <- seq.POSIXt(
      from = as.POSIXct(
        data %>%
      filter(home == home_num &
               location %in% c('living', 'bedroom', 'kitchen'))%>%
        pull(datetime) %>% min(), tz = 'UTC'),

      to =  as.POSIXct( data %>%
      filter(home == home_num &
               location %in% c('living', 'bedroom', 'kitchen'))%>%
        pull(datetime) %>% max(), tz = 'UTC'),
      by = '5 min')

    
    #calculate missingness of data in year for given location
    year_data_avail <-
      length(data_year %>% pull(datetime) %>% unique())/
      length(all_dates_year)




    # make df for recorded values in season for given location
    data_season <- data_year %>%
      # choose season date range
      filter(
        datetime >= ymd(monitor_season[1]) &
          datetime <= ymd(monitor_season[2])
      )


    # make column of all dates (increments of 5 min) within season
    # including all locations
    all_dates_season <- seq.POSIXt(from = as.POSIXct( monitor_season[1], tz = 'UTC'),
                                   to =  floor_date(as.POSIXct( monitor_season[2],
                                                                tz = 'UTC')+24*60*60-1,
                                                    unit = '5 min'),
                                   by = '5 min')


    # calculate missingness of data points in the season
    season_data_avail <-
      length(data_season %>% pull(datetime) %>% unique())/
      length(all_dates_season)


    if(all_time == TRUE) {

      # stop if missing 25% of data in year
      if(year_data_avail < 0.75) {
        output <<- na.result(
          paste0('year_missing_',
                 signif((1-year_data_avail)*100, 2),
                 '%'),
          period_label
        )
        stop()
      }

      entire <- data_year

    } else {

          # stop if missing 25% of data in season
    if(season_data_avail < 0.75) {
      output <<- na.result(
        paste0('season_missing_',
               signif((1-season_data_avail)*100, 2),
               '%'),
        period_label
      )
      stop()
    }

      entire <- data_season
    }

    # make a fitted distribution for data assuming log-normal distribution
    entire_fit <- fitdistrplus::fitdist(pull(entire, metric),
                                        "lnorm", method = 'mle') %>%
      suppressWarnings() %>%
      tryCatch(
        error = function(e)   {
          output <<- na.result( 'no_fit_lnorm_entire',
                                period_label
          )
          stop()
        }
      )
    # make function of continuous distribution from the parameters
    # calculated in the fitted curve
    # note that meanlog and sdlog in dlnorm correspond to the mean and sd,
    # so you do not have to take log of them yourself


    log_fit_entire <- function(x) {

      fit_value <- dlnorm(x, meanlog = entire_fit$estimate['meanlog'],
                          sdlog = entire_fit$estimate['sdlog'])

      # make sure R does not convert to 0 if result is very small number
      ifelse( abs(fit_value) > .Machine$double.xmin,
              fit_value,
              .Machine$double.xmin)
    } %>%
      tryCatch(
        error = function(e)   {
          output <<- na.result( 'lnorm_param_entire',
                                period_label
          )
          stop()
        }
      )

    # function to take a running window of all short sampling periods possible
    # within the longer montirong period,



    kld.avg <- function(days) { # days = number of days in short samping period

      # for testing
      # y<- all_dates_season[(159*24*12):((159+28)*24*12)]

      a <-
        runner(
          all_dates_season,
          k = days*24*12, # number of 5-min periods in short sampling period
          # only evaluate windows that start at 12AM
          # and windows that are full (ignore partial windows at start)
          at = seq(days*24*12, length(all_dates_season), 24*12),
          f = function(y) { # y = window, vector (length = k) of specified days per iteration
            
            # filter out only given room
            # and the days specified by the days in window y
            sample <- data_season %>%
              filter( location == location_type & datetime %in% y)
            
            # omit sampling period if missing more than
            # 25 % of sampling period
            if(nrow(sample) < 0.75 * days*24*12) {
              
              a <- rep(NA, 2) %>% as.integer()
              
              
              
              # if not missing 25% of sampling period...
            } else {
              
              # fit a lognormal distribution to sample and extract
              # logmean and logsd values
              sample_fit <- fitdistrplus::fitdist(pull(sample, metric),
                                                  "lnorm", method = 'mle') %>%
                suppressWarnings() %>% # warnings not a problem in this case
                tryCatch(
                  error = function(e)   {
                    output <<- na.result( 'no_fit_lnorm_sample',
                                          period_label,
                                          days)
                    stop()
                  }
                )
              
              a <- c(sample_fit$estimate['meanlog'], sample_fit$estimate['sdlog'])
              
            }
            
            # return values for one running window
            a <- as.data.frame(a)
            colnames(a) <- y[1] # give name to column just to suppress "new name" message
            
            a
          }
        )
      
      # bind all elements of list into a dataframe
      a <- bind_cols(a)
      
      
      
      n_samples_possible <- ncol(a)
      # omit columns that have NA values (didn't have enough data)
      # use if statement to avoid changing
      # structure of single column to vector
      if(ncol(a)>1) a <- a[ , colSums(is.na(a)) == 0]
      
      # count number of sampling periods created (columns)
      n_samples <- ncol(a)
      
      # calculate proportion of samples that had sufficient data
      # for given short sampling frame length
      n_samp_avail <- n_samples/n_samples_possible
      
      
      
      
      
      # apply KLD between the each short sample period and
      # the season or entire monitoring period individually
      
      a <- lapply(colnames(a), function(x) {
        
        a <- a %>% pull(x)
        
        log_fit_sample <- function(x) {
          
          fit_value <- dlnorm(x, meanlog = a[1], sdlog = a[2])
          
          # make sure R does not convert to 0 if result is very small number
          ifelse( abs(fit_value) > .Machine$double.xmin,
                  fit_value,
                  .Machine$double.xmin)
        } %>%
          tryCatch(
            error = function(e)   {
              output <<- na.result( 'lnorm_param_sample',
                                    period_label,
                                    days
              )
              stop()
            }
          )
        
        # find kld from two continuous functions
        # bounded from 0 to Inf
        kld_fit <- function(x) {
          
          fit_ratio_value <- (log_fit_sample(x))/(log_fit_entire(x))
          
          # make sure R does not convert to 0 if result is very small number
          fit_ratio_value <- ifelse( abs(fit_ratio_value) > .Machine$double.xmin,
                                     fit_ratio_value,
                                     .Machine$double.xmin)
          
          log_fit_sample(x)*log(fit_ratio_value)
        }
        
        
        # find kld value
        integrate(kld_fit, lower = 0,
                  # increase tolerance to avoid "divergent" message
                  rel.tol = .Machine$double.eps^0.5,
                  upper = Inf)$value %>%
          tryCatch(
            error = function(e)   {
              output <<- na.result( 'integration_issue',
                                    period_label,
                                    days
              )
              stop()
            }
          )
        
      })
      
      

      # average all the resulting KLDs to find average KLD
      # for specified short sampling period length
      # and calculate the standard error


      list('kld_avg' = mean(unlist(a)),
           'kld_sd' = sd(unlist(a)),

           'n_samp_avail' = n_samp_avail)

    }


    # # test kld.avg function
    # test <- kld.avg(16)


    # calculate "max KLD":
    # average kld for short sampling periods
    # of minimum length, "sample_days_min"
    a <- kld.avg(sample_days_min)

    kld_avg_max <- a[['kld_avg']]

    kld_sd_max <- a[['kld_sd']]

    # apply function to find scaled_entropy dataframe for
    # sampling period of length "days2" to range of days in tested_sample_sequence
    entropy_data_list <- lapply(
      tested_sample_sequence,
      function (
        days2 # length of short sampling period
      ) {


        # calculate average KLD for all short sampling period of length "days2"
        a <- kld.avg(days2)
        kld_avg <- a[['kld_avg']]
        kld_sd <- a[['kld_sd']]



        # # calculate the sd for ratio of two means,
        # # assuming independence (no covariance) between sample of given size
        # # and minimum sample size
        # b <- (kld_avg/kld_avg_max)*sqrt(
        #   kld_sd^2/kld_avg^2+
        #     kld_sd_max^2/kld_avg_max^2
        # )


        a <- tibble(
          'coeff' = kld_avg/kld_avg_max, # mean scaled entropy value
          # # values for sd_int assuming kld_avg_min is a set number
          'high_val' = (kld_avg+kld_sd)/kld_avg_max,
          'low_val' = (kld_avg-kld_sd)/kld_avg_max,
          # # # values for sd_int assuming two independent samples
          # 'high_val2' = kld_avg/kld_avg_max+b,
          # 'low_val2' = kld_avg/kld_avg_max-b,
          'sample_length' = days2,
          'monitor_season' = period_label,
          'n_samp_avail' = a[['n_samp_avail']], # amount of samples with sufficient data
          'error' = 'none'
        )

        a

      }
    )


    # bind all into a dataframe
    a <- bind_rows(entropy_data_list)

    a # scaled_entropy for one date range

  } %>%
    # if season results in an error, return the error message in a df
    tryCatch(error = function(e) output)

  # find scaled_entropy for all short smpling lengths in all apecified time ranges

  a <- lapply(date_ranges_entire, scaled.entropy.season)


  # bind all into a dataframe
  entropy_data_season <- bind_rows(a) %>%
    mutate(method = 'shape',
           home = home_num,
           metric = metric,
           location = location_type,
           period_compare = ifelse(all_time == TRUE, 'year', 'season'))


  entropy_data_season  # scaled_entropy for all specified date ranges

}




# test function--------------------------
start <- Sys.time()

test<- scaled.entropy.table.shape('004', 'pm25',

                           date_ranges_entire = list(
                             c('2020-12-01', '2020-12-31'),
                                                     c('2020-11-01',
                                                       '2020-11-30')),
                           tested_sample_sequence = c(4,5,6),
                           location_type = 'living',
                           sample_days_min = 3,
                           all_time = TRUE)



end <- Sys.time()
run2 <- end- start

# # look into bootstrapping
# a_orig # all kld results
# boot1 <- sample(a_orig, size = 2, replace = T)
# boot <- lapply(1:20, function(i) sample(a_orig, replace = T))


```


```{r shape_data_maker_range, eval = FALSE}
# import energy cluster dataframe for all homes
energy_cluster_df <- read_csv('../sense/csv_created_sense/energy_cluster_df.csv')

# import summary df of how many lags required
# before autocorrelation was insignificant
lag_summary_df <- read_csv('./csv_created/lag_summary.csv')

# make functions to look up starting and ending date of
# energy cluster period based on home
 pick.start <- function(x_home, x_cluster) {
   energy_cluster_df %>%
   filter(home == x_home & cluster_type == x_cluster) %>%
   pull(start_date) %>% as.character()
 }
 
  pick.end <- function(x_home, x_cluster) {
   energy_cluster_df %>%
   filter(home == x_home & cluster_type == x_cluster) %>%
   pull(end_date) %>% as.character()
  }
  
  # fun to pick minimum sample length based on metric
  pick.sample.min <- function(x_metric) {
    lag_summary_df %>%
      # use the pooled median of all clusters
      filter(metric == x_metric, energy_cluster == 'total') %>%
      pull(median)
  }
  # # test functions
   # pick.start('006', 'heat')
  # pick.end('004', 'heat')


# test function for multiple homes
test_kit <- lapply('009', function(x_home) {

  start_date <- pick.start(x_home, clust)
  end_date <- pick.end(x_home, clust)
  sample_min <- pick.sample.min(met)
  sample_max <- if_else(as.numeric(as.Date(end_date) - as.Date(start_date))>=28,
                        28, as.numeric(as.Date(end_date) - as.Date(start_date)))

  a <-
    scaled.entropy.table.shape(
      x_home, metric = met,
      date_ranges_entire = list(c(start_date,
                                  end_date)),
      tested_sample_sequence =
        seq(sample_min,sample_max,8),
      sample_days_min = sample_min,
      location_type = loc,
      all_time = all_time_choice) %>%
    mutate(energy_cluster = clust)
  
  # # give a variable name to the created data
  # assign(paste('home', x, met, sep = '_'),
  #          a,
  #          envir = .GlobalEnv)
  a
}
) %>%
  bind_rows()




# make csvs--------------------
  all_time_choice <- TRUE
    met <- 'pm25'
    loc <- 'living'
    clust <- 'heat' # can do multiple
   
    
    
    # apply function to all clusters if desired
    {lapply(clust, function(cluster_type) {
      
      rep_data <-  lapply(homes_all, function(x_home) {
        
        start_date <- pick.start(x_home, cluster_type)
        end_date <- pick.end(x_home, cluster_type)
        sample_min <- pick.sample.min(met)
        sample_max <- if_else(
          as.numeric(as.Date(end_date) - as.Date(start_date))>=28,
          28,
          as.numeric(as.Date(end_date) - as.Date(start_date))
          )
        
        scaled.entropy.table.shape(
          x_home, metric = met,
          date_ranges_entire = list(c(start_date,
                                      end_date)),
          tested_sample_sequence =
            seq(sample_min,sample_max,4),
          sample_days_min = sample_min,
          location_type = loc,
          all_time = all_time_choice) %>%
          mutate(energy_cluster = cluster_type) 
      }
      ) %>%
        bind_rows()
      
      # give a variable name to the created data
      assign(paste(met, loc, cluster_type, 'rep_data_shape', sep = '_'),
             rep_data,
             envir = .GlobalEnv)
    }
    )
    }
    

  entropy_shape_df <- bind_rows(
    pm25_bedroom_heat_rep_data_shape,
    pm25_living_heat_rep_data_shape,
    pm25_kitchen_heat_rep_data_shape,
    pm25_bedroom_shoulder_rep_data_shape,
    pm25_living_shoulder_rep_data_shape,
    pm25_kitchen_shoulder_rep_data_shape,
    pm25_bedroom_ac_rep_data_shape,
    pm25_living_ac_rep_data_shape,
    pm25_kitchen_ac_rep_data_shape,
    # voc_bedroom_heat_rep_data_shape,
    # voc_living_heat_rep_data_shape,
    # voc_kitchen_heat_rep_data_shape,
    # voc_bedroom_shoulder_rep_data_shape,
    # voc_living_shoulder_rep_data_shape,
    # voc_kitchen_shoulder_rep_data_shape,
    # voc_bedroom_ac_rep_data_shape,
    # voc_living_ac_rep_data_shape,
    # voc_kitchen_ac_rep_data_shape
  )

  
  # # for binding new rows to old rows
# a <- read_csv(file = './csv_created/representativeness_data/rep_data_shape.csv')
# 
# a <- rbind(a, entropy_shape_df)
# 
# write_csv(a, file =
#             paste0('./csv_created/representativeness_data/rep_data_shape_', Sys.Date(), '.csv'))

# make csv of all season data
write_csv(entropy_shape_df, file =
            paste0('./csv_created/representativeness_data/rep_data_shape_', Sys.Date(), '.csv'))






```


### Plot Average Scaled Entropy for Homes

```{r shape_plotter}

# import entropy data
entropy_shape_df <- read_csv(file = './csv_created/representativeness_data/rep_data_shape.csv')


# # plot results for one home
# 
# a <- entropy_shape_df %>% filter(home == '009' & period_compare == 'year' )%>%
#   group_by(sample_length, metric, energy_cluster) %>%
#   summarize(coeff = mean(coeff, na.rm = TRUE)) %>% # average klds from all rooms
#   ungroup()
# 
# 
# ggplot(aes(x = sample_length, y = coeff, color = energy_cluster),
#        data = a)+
#   geom_line()+
#   geom_point(size = 1)+
#   xlab('Sample Length, days')+
#   ylab('Scaled Relative Entropy')+
#   geom_hline(yintercept=0.1, linetype="dashed", 
#                 color = "black", size=0.5)+
#   coord_cartesian(ylim = c(0,1)) +
#   facet_wrap(vars(metric))


# all_homes------------------------------------------------------
entropy.plot <- function (
  hm,
  # omit a sampling period of given length
  # if this proportion of possible samples
  # were not availabel in data
  metrics = c('pm25', 'voc'),
  sample_omit = 0.5,
  data = entropy_shape_df
) {
  
  # for one home:
  a <- data %>%
    filter(home == hm &
             metric %in% metrics &
             n_samp_avail >= sample_omit) %>%
    group_by(sample_length, metric, energy_cluster) %>%
    # average klds from all rooms
    summarize(coeff = mean(coeff, na.rm = TRUE), .groups = 'drop') 

  if(nrow(a) == 0) {
    plot <- NULL # return NULL if no data for home/metric/cluster combo
  }else {
    
    # plot results
    plot <- ggplot(aes(x = sample_length, y = coeff,
                       color = energy_cluster), data = a)+
      geom_line()+
      geom_point(size = 1)+
      xlab('Sample Length, days')+
      ylab('Scaled Relative Entropy')+
      geom_hline(yintercept=rep_threshold_seq, linetype="dashed", 
                 color = "black", size=0.5)+
      coord_cartesian(ylim = c(0,1))+
      ggtitle(paste('Home',hm)) +
      facet_wrap(vars(metric)) +
      scale_x_continuous(breaks=seq(0,28,7))
    # geom_ribbon(aes(ymin = low_val, ymax = high_val, fill = monitor_period),
    #             alpha = 0.3, color = NA)
  }
  plot
}



# # test for one home
# entropy.plot.metrics.seasons(a, '003', c('pm25_ihs'))


# make plots for all homes
lapply(homes_all,
       entropy.plot)



```

### Testing Different Entropy Thresholds

### Effect on when average Scaled Entropy crosses below threshold

```{r threshold_plots_shape}

# months--------------------------------------------------------------

# # make dataframe of values for minimum sample length (in days) where entropy is less than 0.1
# entropy_cutoffs <- entropy_all %>%
#   filter(n_samp_avail >0.5 & method == 'shape') %>%
#   filter(
#     monitor_period %in% month.abb
#     ) %>%
#   filter(coeff <= threshold) %>% 
#   group_by(home, metric, monitor_period) %>%
#   summarise_at(vars(sample_length), min) %>%
#   ungroup()
# 
# # fill in missing combinations so bar chart will have bars of constant width
# a <- expand.grid(home=unique(entropy_cutoffs$home),
#                  metric=unique(entropy_cutoffs$metric),
#                  monitor_period=unique(entropy_cutoffs$monitor_period))%>%
#   data.frame() %>%
#   left_join(entropy_cutoffs)
# 
# 
# 
# # bar chart of when threshold was reached for each home
# ggplot(aes(x = home, y = sample_length, fill = metric), data = a)+
#   geom_col(position = 'dodge')+
#   facet_wrap(vars(monitor_period), nrow = 2)
# 
# 
# 
# 
# 
# 
# 
# # make dataframe of values for minimum sample length (in days)
# # where entropy is less than threshold value
# threshold.freq.table <- function (threshold) {
#   
#   
#   entropy_cutoffs <- entropy_all %>%
#   filter(n_samp_avail >0.5 & method == 'shape') %>%
#   filter(
#     monitor_period %in% month.abb
#     ) %>%
#   filter(coeff <= threshold) %>% 
#   group_by(home, metric, monitor_period) %>%
#   summarise_at(vars(sample_length), min) %>%
#   ungroup()
#     
# 
#   # fill in missing combinations so bar chart will have bars of constant width
# a <- expand.grid(home=unique(entropy_cutoffs$home),
#                  metric=unique(entropy_cutoffs$metric),
#                  monitor_period=unique(entropy_cutoffs$monitor_period))%>%
#   data.frame() %>%
#   left_join(entropy_cutoffs) %>%
# # count frequency of home-month instances when threshold was reached for each sample length
#   group_by(sample_length, metric) %>%
#   summarise(freq = n()) %>%
#   ungroup() %>%
#   filter(!is.na(sample_length)) %>%
#   mutate(thresh = as.factor(threshold))
# }
# 
# 
# # apply function to all thresholds in threshold_seq
#  a <- lapply(threshold_seq, threshold.freq.table)
#  a <- do.call(rbind, a)
# 
#  # ensure there weren't a significant amount of omissions for any metric type
#  # (because the threshod may never have been reached)
#  
#  # test <- a %>%
#  #   group_by(metric) %>%
#  #   summarise(n = sum(freq)) %>%
#  #   ungroup()
# 
#  # histogram of when threshold was reached for all home/month combos
# ggplot(aes(x = sample_length, fill = thresh), data = a)+
#   geom_density(alpha = 0.5)+
#   facet_wrap(vars(metric))+
#   xlab("Sample Length Required to Meet Representativeness Threshold, days")+
#   scale_x_continuous(breaks = c(7,14,21,28))


# maybe limit to Aug-Feb (7 months), 15 homes (4 metrics) for now?


# seasons--------------------------------------------------------------


# make dataframe of values for minimum sample length (in days)
# where entropy is less than threshold value
threshold.freq.table.seasons <- function (threshold) {
  
  
  a <- entropy_all %>%
  filter(n_samp_avail >0.5 & method == 'shape') %>%
  filter(
    monitor_period %in% c('fall', 'winter')
    ) %>%
  filter(coeff <= threshold) %>% 
  group_by(home, metric, monitor_period) %>%
  summarise_at(vars(sample_length), min) %>%
  ungroup() %>%
    mutate('thresh' = threshold)
    

}


# apply function to all thresholds in threshold_seq
 a <- lapply(threshold_seq, threshold.freq.table.seasons)
 a <- do.call(rbind, a)

 # ensure there weren't a significant amount of omissions for any metric type
 # (because the threshod may never have been reached)
 
 # test <- a %>%
 #   group_by(metric) %>%
 #   summarise(n = sum(freq)) %>%
 #   ungroup()
 
   lvls <- levels(as.factor(a$thresh))
rows <- by(a, a$thresh, function(x) nrow(x))
labels <- paste(lvls,", n=",as.integer(rows),sep="")

 # histogram of when threshold was reached for all home/month combos
ggplot(aes(x = sample_length, fill = as.factor(thresh)), data = a)+
  geom_density(alpha = 0.5)+
  facet_wrap(vars(metric))+
  xlab("Sample Length Required to Meet Representativeness Threshold, days")+
  scale_x_continuous(breaks = c(7,14,21,28))+
  scale_fill_discrete(labels = labels)+
  annotate('text', x = 14, y = 0.15, label = '*max n = 16 (number of homes)')



```

### Effect on when average Scaled Entropy crosses below threshold


```{r threshold_plots_time, eval = FALSE}

# months--------------------------------------------------------------

# make dataframe of values for minimum sample length (in days) where entropy is less than 0.1
entropy_cutoffs <- entropy_all %>%
  filter(n_samp_avail >0.5) %>%
  filter(
    monitor_period %in% month.abb
    ) %>%
  filter(coeff <= thershold) %>% 
  group_by(home, metric, monitor_period) %>%
  summarise_at(vars(sample_length), min) %>%
  ungroup()

# fill in missing combinations so bar chart will have bars of constant width
a <- expand.grid(home=unique(entropy_cutoffs$home),
                 metric=unique(entropy_cutoffs$metric),
                 monitor_period=unique(entropy_cutoffs$monitor_period))%>%
  data.frame() %>%
  left_join(entropy_cutoffs)



# bar chart of when threshold was reached for each home
ggplot(aes(x = home, y = sample_length, fill = metric), data = a)+
  geom_col(position = 'dodge')+
  facet_wrap(vars(monitor_period), nrow = 2)







# make dataframe of values for minimum sample length (in days)
# where entropy is less than threshold value
threshold.freq.table <- function (threshold) {
  
  
  entropy_cutoffs <- entropy_all %>%
  filter(n_samp_avail >0.5) %>%
  filter(
    monitor_period %in% month.abb
    ) %>%
  filter(coeff <= threshold) %>% 
  group_by(home, metric, monitor_period) %>%
  summarise_at(vars(sample_length), min) %>%
  ungroup()
    

  # fill in missing combinations so bar chart will have bars of constant width
a <- expand.grid(home=unique(entropy_cutoffs$home),
                 metric=unique(entropy_cutoffs$metric),
                 monitor_period=unique(entropy_cutoffs$monitor_period))%>%
  data.frame() %>%
  left_join(entropy_cutoffs) %>%
# count frequency of home-month instances when threshold was reached for each sample length
  group_by(sample_length, metric) %>%
  summarise(freq = n()) %>%
  ungroup() %>%
  filter(!is.na(sample_length)) %>%
  mutate(thresh = as.factor(threshold))
}


# apply function to all thresholds in threshold_seq
 a <- lapply(threshold_seq, threshold.freq.table.seasons)

 a <- do.call(rbind, a)

 # ensure there weren't a significant amount of omissions for any metric type
 # (because the threshod may never have been reached)
 
 # test <- a %>%
 #   group_by(metric) %>%
 #   summarise(n = sum(freq)) %>%
 #   ungroup()

 # histogram of when threshold was reached for all home/month combos
ggplot(aes(x = sample_length, fill = thresh), data = a)+
  geom_density(alpha = 0.5)+
  facet_wrap(vars(metric))+
  xlab("Sample Length Required to Meet Representativeness Threshold, days")+
  scale_x_continuous(breaks = c(7,14,21,28))


```


---
# Testing Functions

## integration functions and normality of data
```{r integration_and_normality}

# 
# # define variables for testing
# date_ranges_entire <- list(c('2020-10-13','2021-04-22'))
# tested_sample_sequence <- seq(3,28,4)
# sample_days_min <- 3
# home_num <- '004'
# location_type <- 'living'
# data <- omni_data
# monitor_season <- date_ranges_entire[1]
# metric <- 'pm25'
# days <- 16
# days2 <- 16
# all_time <- TRUE
# 
# 
# date_ranges_entire <- NULL
# tested_sample_sequence <- NULL
# sample_days_min <- NULL
# home_num <- NULL
# location_type <- NULL
# monitor_season <- NULL
# monitor_period <- NULL
# metric <- NULL
# days <- NULL
# days2 <- NULL
# all_time <- NULL
# 
# 
# 
# # test function --------------------------
# scaled.entropy.table.shape <- function(home_num, metric,
#                                        
#                                        # date range of entire monitoring period (if not month)
#                                        # in the form: list(
#                                        #                    c('YYYY-MM-DD', 'YYYY-MM-DD'),
#                                        #                    c('YYYY-MM-DD', 'YYYY-MM-DD')
#                                        #                  )
#                                        # data in start and end day included
#                                        date_ranges_entire,
#                                        tested_sample_sequence, # sequence of tested short sampling period lengths (in days)
#                                        sample_days_min, # amount of days in shortest theoretical sample
#                                        location_type = 'living',
#                                        data = omni_data) {
#   
#   
#   if(metric %in% c('pm25', 'voc')) {
#     # define LOD of metric to be 1/2 minimum detected (non-zero) value
#     LOD <- data %>%
#       filter(!!sym(metric)!=0) %>%
#       pull(all_of(metric))%>%
#       min()
#   }
#   
# 
#     
#     
#     
#     # must unlist the listed range that is used in lapply funnction
#     monitor_period <- unlist(monitor_period)
#     
#     #define label for later use in table
#     period_label <- paste(as.Date(monitor_period[1]), '-',
#                           as.Date(monitor_period[2]))
#     
#     # warn that times will be rounded to full day for entire monitoring period
#     if(any(
#       c(
#         second(as.POSIXct(monitor_period[1])),
#         minute(as.POSIXct(monitor_period[1])),
#         hour(as.POSIXct(monitor_period[1])),
#         
#         second(as.POSIXct(monitor_period[2])),
#         minute(as.POSIXct(monitor_period[2])),
#         hour(as.POSIXct(monitor_period[2]))) > 0
#     )) stop('Times cannot be specifed in date range endpoints.')
#     
# 
#     
#     # make df of entire monitoring period (approx. a year) 
#     data_year <- data %>%
#       filter(home == home_num & location == location_type) %>%
#       dplyr::select(c(datetime, all_of(metric)))
#     
#     
#     if(metric %in% c('pm25', 'voc')) {
#       # convert 0 values to LOD/2 in to allow for log-normal distrib. estimation
#       data_year <- data_year %>%
#         mutate_at(all_of(metric), function(x) ifelse(x==0, LOD/2, x))
#     }
#     
#     # make a vector of dates when value was recorded in year
#     date_col_year <- data_year$datetime
#     
#     
#     
#     # make column of all dates (increments of 5 min) within year
#     
#     all_dates_year <- seq.POSIXt(
#       from = as.POSIXct( data_year%>%pull(datetime) %>% min(), tz = 'UTC'),
#       
#       to =  as.POSIXct( data_year%>%pull(datetime) %>% max(),
#                         tz = 'UTC'),
#       by = '5 min')
#     
#     #calculate missingness of data in year
#     year_data_avail <- length(date_col_year)/length(all_dates_year)
#     
#     
#     
#     # make column of specified metric for one home, one room
#     data_season <- data_year %>%
#       # choose season date range
#       filter(
#         datetime >= ymd(monitor_season[1]) &
#           datetime <= ymd(monitor_season[2])
#       )
#     
#     
#     # make a vector of dates when value was recorded
#     date_col_season <- data_season$datetime
#     
#     
#     
#     # make column of all dates (increments of 5 min) within season
#     all_dates_season <- seq.POSIXt(from = as.POSIXct( monitor_season[1], tz = 'UTC'),
#                                    to =  floor_date(as.POSIXct( monitor_season[2],
#                                                                 tz = 'UTC')+24*60*60-1,
#                                                     unit = '5 min'),
#                                    by = '5 min')
#     
#     
#     # calculate missingness of data points in the season
#     season_data_avail <- length(date_col_season)/length(all_dates_season)
#     
#     # stop if missing 25% of data in season
#     if(season_data_avail < 0.75) stop(paste('missing ',
#                                             signif((1-season_data_avail)*100, 2),
#                                             '% of daily data in season,',
#                                             period_label))
#     
#     if(all_time == TRUE) {
#       
#       # stop if missing 25% of data in year
#       if(year_data_avail < 0.75) stop(paste('missing ',
#                                             signif((1-year_data_avail)*100, 2),
#                                             '% of daily data in entire monitor period'))
#       
#       entire <- data_year
#       
#     } else { 
#       entire <- data_season
#     }
#     
#     entire_fit <- fitdistrplus::fitdist(pull(entire, metric),
#                                         "lnorm", method = 'mle') %>%
#       suppressWarnings()
# 
#     
#     # make function of continuous distribution from the parameters
#     # calculated in the fitted curve
#     # note that meanlog and sdlog in dlnorm correspond to the mean and sd,
#     # so you do not have to take log of them yourself
#     
#     
#     log_fit_entire <- function(x) {
#       
#       fit_value <- dlnorm(x, meanlog = entire_fit$estimate['meanlog'],
#                           sdlog = entire_fit$estimate['sdlog'])
#       
#       # make sure R does not convert to 0 if result is very small number
#       ifelse( abs(fit_value) > .Machine$double.xmin,
#               fit_value,
#               .Machine$double.xmin)
#     }
#     
#     # test goodness of fit for log_normal
#     
# 
#     ggplot()+
#       geom_line(aes(x = x_vals, y = log_fit_entire(x_vals)))+
#       geom_histogram(aes(x = entire %>% pull(metric), y = ..density..),
#                      binwidth = 0.5, alpha = 0.6)+
#       coord_cartesian(xlim = c(0, 10))
#     
#     ggqqplot(entire %>% pull(metric)%>% log())
#     
#     # too many points for shapiro
#     shapiro.test(entire %>% pull(metric) %>% log())
#     
#     # function to take a running window of all short sampling periods possible
#     # within the longer montirong period,
#     
#       # for testing
#       # y<- all_dates_season[(158*24*12):((158+28)*24*12)]
#       
#       a <- 
#         runner(all_dates_season,
#                k = days*24*12, # number of 5-min periods in short sampling period
#                # only evaluate windows that start at 12AM
#                # and windows that are full (ignore partial windows at start)
#                at = seq(days*24*12, length(all_dates_season), 24*12),
#                f = function(y) { # y = window, vector (length = k) of specified days per iteration
#                  
#                  sample <- data_season %>%
#                    
#                    # filter out only the days specified by the days in window y
#                    filter(datetime %in% y)
#                  
#                  
#                  # omit sampling period if missing more than
#                  # 25 % of sampling period
#                  if(nrow(sample) < 0.75 * days*24*12) { 
#                    
#                    a <- list(rep(NA, 2) %>% as.integer(), NULL)
#                    
#                    
#                    
#                    # if not missing 25% of sampling period...
#                  } else { 
#                    
#                    sample_vals <- pull(sample, metric)
#                    # fit a lognormal distribution to sample and extract
#                    # logmean and logsd values
#                    sample_fit <- fitdistrplus::fitdist(pull(sample, metric),
#                                                        "lnorm", method = 'mle') %>%
#                      suppressWarnings() # warnings not a problem in this case
#                    
#                    a <- list( 'estimates' = c(sample_fit$estimate['meanlog'],
#                                           sample_fit$estimate['sdlog']),
#                              'sample' = sample_vals)
#                    
#                    
#                  }
#                  
#                  # return values for one running window
#  
#                  a
#                }
#         )
# 
#       
#       
#       # extract paramter estimates
#       test_estimates <- lapply(seq(1,length(a)-1, 2), function(x) {
#         tibble( 'meanlog' = a[[x]][1],
#                 'sdlog' = a[[x]][2])
#       }
#       )
#       # remove samples that are NA
#       test_estimates <- test_estimates[-which(sapply(test_estimates,
#                                                     function(x) sum(is.na(x)) != 0))]
# 
#       
#       
#       # extract sample data
#       test_samples <- lapply(seq(2,length(a), 2), function(x) a[[x]])
#       # remove NULL samples
#       test_samples <- test_samples[-which(sapply(test_samples, is.null))]
#       
#       
#       # make function from log-fit parameters
#       log_fit_sample <- function(samp_num, x) {
#         
#         fit_value <- dlnorm(x, meanlog = test_estimates[[samp_num]]%>% pull(meanlog),
#                             sdlog = test_estimates[[samp_num]]%>% pull(sdlog))
#         
#         # make sure R does not convert to 0 if result is very small number
#         ifelse( abs(fit_value) > .Machine$double.xmin,
#                 fit_value,
#                 .Machine$double.xmin)
#       }
#       
#       # find kld from log fit functions of sample and entire period
#       # bounded from 0 to Inf
#       kld_fit <- function(samp_num,x) {
#         
#         fit_ratio_value <- (log_fit_sample(samp_num,x))/(log_fit_entire(x))
#         
#         # make sure R does not convert to 0 if result is very small number
#         fit_ratio_value <- ifelse( abs(fit_ratio_value) > .Machine$double.xmin,
#                                    fit_ratio_value,
#                                    .Machine$double.xmin)
#         
#         log_fit_sample(samp_num,x)*log(fit_ratio_value)
#       }
#       
#       
#      
# 
#       
#       # # check for outliers
#       # ## with raw data
#       # ggplot()+
#       #   geom_boxplot(aes(y = test_samples[[sample_number]]))
#       # 
#       # ## with logged data
#       # ggplot()+
#       #   geom_boxplot(aes(y = test_samples[[sample_number]] %>% log()))
# 
#       # look at samples vs fitted log distribution and dist for entire period
#       ggplot()+
#         geom_line(aes(x = x_vals, y = log_fit_sample(sample_number, x_vals)),
#                   color = 'black')+
#         geom_line(aes(x = x_vals, y = log_fit_entire( x_vals)),
#                   color = 'red')+
#       geom_histogram(aes(x = test_samples[[sample_number]],
#                          y = ..density..),
#                      binwidth = 0.5, alpha = 0.6, color = 'grey')+
#         coord_cartesian(xlim = c(0, 12))
#       
#       
# 
#       # apply shapiro test to logged values and
#       # see if samples are considered normal      
#       shap_test_df <- lapply(lapply(test_samples, log), 
#                              function(x) shapiro.test(x)$p.value)
# 
#       x_vals <- seq(0, 20, 0.1)
#       
#       sample_number <- 92
#       
#       # check kld function
#       ggplot()+
#         geom_line(aes(x = x_vals, y = kld_fit(sample_number, x_vals)))
#       
#       # check that the sample can be integrated
#       integrate(kld_fit, lower = 0, upper = Inf, samp_num = sample_number)$value
#       
#       # # different integration method, works near zero, but much slower
#       # cubintegrate(kld_fit, lower = 0, upper = Inf, method = "pcubature", 
#       #              samp_num = sample_number)
# 
#       
#       
#       integrate.kld <- function(x) {
#         
#         sapply(x, function(y) {
#           temp <- integrate(f = kld_fit, lower = 0, samp_num = y,
#                             upper = Inf,
#                             rel.tol = 1e-15)
#           temp$value
#         }
#         ) %>% as.data.frame() %>% rename('integral' = '.') %>%
#           bind_cols('sample' = x) %>%
#           pull(integral)
#       }
#       
#       
#       
#       # test integrate function for multiple inputs
#       int <- integrate.kld(c(1:160))
#       
#       
#       ggplot(data = NULL, aes(x = x_vals))+
#         # geom_line(aes(y = integrate.kld(x_val)))+
#         # geom_line(aes(y = kld_fit(x_val)))+
#         geom_line(aes(y = log_fit_sample(x_vals)))+
#         geom_line(aes(y = log_fit_entire(x_vals)))+
#         
#         scale_x_log10()
#       
#   
# }
# 

```

## old function when rooms were only pooled for entire monitoring period

```{r function_shape_pooled}

# 
# # give a row of NA values with identifers to help with identifying
# # causes of errors in function
# na.result <- function(error, season = NA, sample_days = NA, samp_avail = NA) {
#   tibble(
#   'coeff' = NA,
#   'high_val' = NA,
#   'low_val' = NA,
#   'sample_length' = sample_days, # sample_length,
#   'monitor_season' = season, # period_label,
#   'n_samp_avail' = samp_avail, # sample_available, # amount of samples with sufficient data
#   'error' = error
# )
# }
# 
# 
# # calculate kld for continuous probability distributions
# 
# # define variables for testing-----------------------
# date_ranges_entire <- list(c('2020-11-01','2021-02-21'))
# tested_sample_sequence <- seq(3,28,8)
# sample_days_min <- 3
# home_num <- '009'
# data <- omni_data
# monitor_season <- date_ranges_entire[1]
# metric <- 'pm25'
# days <- 16
# days2 <- 16
# all_time <- TRUE
# 
# # remove variables after done testing
# rm(tested_sample_sequence,
#    date_ranges_entire,
# sample_days_min,
# home_num,
# monitor_season,
# monitor_period,
# metric,
# days,
# days2,
# all_time,
# data,
# data_season,
# data_year,
# entire,
# date_col_season,
# date_col_year,
# all_dates_season,
# all_dates_year
# )
# 
# 
# 
# # entropy function minutely data ----------------------------
# 
# 
# 
#     
# # function to calculate scaled entropy for a house
# # during multiple different time periods
# scaled.entropy.table.shape <- function(home_num, metric,
#                                        
#                                        # date range of entire monitoring period
#                                        # in the form: list(
#                                        #                    c('YYYY-MM-DD', 'YYYY-MM-DD'),
#                                        #                    c('YYYY-MM-DD', 'YYYY-MM-DD')
#                                        #                  )
#                                        # data in start and end day included
#                                        date_ranges_entire,
#                                        tested_sample_sequence, # sequence of tested short sampling period lengths (in days)
#                                        all_time = FALSE, # if TRUE, compare each short sampling period to entire monitoring year of home
#                                        sample_days_min, # amount of days in shortest theoretical sample
#                                        
#                                        data = omni_data) {
#   
#   output <<- na.result(NA) # clear error output in case it was triggered previously
#   
#   if(metric %in% c('pm25', 'voc')) {
#     # define LOD of metric to be 1/2 minimum detected (non-zero) value
#     LOD <- data %>%
#       filter(!!sym(metric)!=0) %>%
#       pull(all_of(metric))%>%
#       min()
#   }
#   
#   
#   # function to find scaled_entropy for samples in a given monitor season
#   
#   scaled.entropy.season <- function(monitor_season) {
#     
#     
#     # must unlist the listed range that is used in lapply funnction
#     monitor_season <- unlist(monitor_season)
#     
#     # stop if monitoring season is empty
#     if(is_empty(monitor_season)) {
#       output <<- na.result('likely_no_cluster')
#       stop()
#     } 
#     
# 
#     #define label for later use in table
#     period_label <- paste(as.Date(monitor_season[1]), '-',
#                           as.Date(monitor_season[2]))
#     
#     # warn that times will be rounded to full day for entire monitoring period
# 
#     if(
#         any( c(
#         second(as.POSIXct(monitor_season[1])),
#         minute(as.POSIXct(monitor_season[1])),
#         hour(as.POSIXct(monitor_season[1])),
#         
#         second(as.POSIXct(monitor_season[2])),
#         minute(as.POSIXct(monitor_season[2])),
#         hour(as.POSIXct(monitor_season[2]))) > 0
#     )
#     ) {
#       output <<- na.result('incorrect_date',
#                            period_label)
#       stop()
#     }
# 
#     
#     # make df of entire monitoring period (approx. a year) 
#     # from all indoor sensors
#     data_year <- data %>%
#       filter(home == home_num & location %in% c('living',
#                                                 'bedroom', 'kitchen')) 
#     
#     if(metric %in% c('pm25', 'voc')) {
#       # convert 0 values to LOD/2 in to allow for log-normal distrib. estimation
#       data_year <- data_year %>%
#         mutate_at(all_of(metric), function(x) ifelse(x==0, LOD/2, x))
#     }
#     
#     # make a vector of dates when a value was recorded in year
#     # including all locations
#     date_col_year <- data_year %>%
#       pull(datetime) %>% unique()
# 
#     
#     # make column of all dates (increments of 5 min) within year
#     # including all locations
#     all_dates_year <- seq.POSIXt(
#       from = as.POSIXct( data_year%>%pull(datetime) %>% min(), tz = 'UTC'),
#       
#       to =  as.POSIXct( data_year%>%pull(datetime) %>% max(),
#                         tz = 'UTC'),
#       by = '5 min')
#     
#     #calculate missingness of data in year including all locations
#     year_data_avail <- length(date_col_year)/length(all_dates_year)
#     
#     
#     
#     # make df for season with all locations
#     data_season <- data_year %>%
#       # choose season date range
#       filter(
#         datetime >= ymd(monitor_season[1]) &
#           datetime <= ymd(monitor_season[2])
#       )
#     
#     
#     # make a vector of dates when value was recorded between all locations
#     date_col_season <- data_season %>%
#       pull(datetime) %>% unique()
#     
#     
#     
#     # make column of all dates (increments of 5 min) within season
#     # including all locations
#     all_dates_season <- seq.POSIXt(from = as.POSIXct( monitor_season[1], tz = 'UTC'),
#                                    to =  floor_date(as.POSIXct( monitor_season[2],
#                                                                 tz = 'UTC')+24*60*60-1,
#                                                     unit = '5 min'),
#                                    by = '5 min')
# 
#     # calculate missingness of data points in the season
#     season_data_avail <- length(date_col_season)/length(all_dates_season)
#     
# 
#     if(all_time == TRUE) {
#       
#       # stop if missing 25% of data in year
#       if(year_data_avail < 0.75) {
#         output <<- na.result(
#           paste0('year_missing_',
#                  signif((1-year_data_avail)*100, 2),
#                  '%'),
#           period_label
#         )
#         stop()
#       }
#       
#       entire <- data_year
#       
#     } else { 
#       
#           # stop if missing 25% of data in season
#     if(season_data_avail < 0.75) {
#       output <<- na.result(
#         paste0('season_missing_',
#                signif((1-season_data_avail)*100, 2),
#                '%'),
#         period_label
#       )
#       stop()
#     }
#       
#       entire <- data_season
#     }
#     
#     # make a fitted distribution for data assuming log-normal distribution
#     entire_fit <- fitdistrplus::fitdist(pull(entire, metric),
#                                         "lnorm", method = 'mle') %>%
#       suppressWarnings() %>%
#       tryCatch(
#         error = function(e)   {
#           output <<- na.result( 'no_fit_lnorm_entire',
#                                 period_label
#           )
#           stop()
#         }
#       )    
#     # make function of continuous distribution from the parameters
#     # calculated in the fitted curve
#     # note that meanlog and sdlog in dlnorm correspond to the mean and sd,
#     # so you do not have to take log of them yourself
#     
#     
#     log_fit_entire <- function(x) {
#       
#       fit_value <- dlnorm(x, meanlog = entire_fit$estimate['meanlog'],
#                           sdlog = entire_fit$estimate['sdlog'])
#       
#       # make sure R does not convert to 0 if result is very small number
#       ifelse( abs(fit_value) > .Machine$double.xmin,
#               fit_value,
#               .Machine$double.xmin)
#     } %>%
#       tryCatch(
#         error = function(e)   {
#           output <<- na.result( 'lnorm_param_entire',
#                                 period_label
#           )
#           stop()
#         }
#       )
#     
#     # function to take a running window of all short sampling periods possible
#     # within the longer montirong period,
#     
# 
#         
#     kld.avg <- function(days) { # days = number of days in short samping period
#       
#       # # for testing
#       # y<- all_dates_season[(4*24*12):((4+16)*24*12)]
#       
#       a <-
#         runner(all_dates_season,
#                k = days*24*12, # number of 5-min periods in short sampling period
#                # only evaluate windows that start at 12AM
#                # and windows that are full (ignore partial windows at start)
#                at = seq(days*24*12, length(all_dates_season), 24*12),
#                f = function(y) { # y = window, vector (length = k) of specified days per iteration
#                  
#                  # pick days specified by the days in window y
#                  sample <- data_season %>%
#                    filter( datetime %in% y)
#                  
#                  # omit sampling period if missing more than
#                  # 25 % of sampling period between all locations
#                  if(length((sample$datetime) %>% unique()) < 0.75 * days*24*12) {
#                    
#                    a <- rep(NA, 2) %>% as.integer()
#                    
#                    
#                    
#                    # if not missing 25% of sampling period...
#                  } else {
#                    
#                    # fit a lognormal distribution to sample and extract
#                    # logmean and logsd values
#                    sample_fit <- fitdistrplus::fitdist(pull(sample, metric),
#                                                        "lnorm", method = 'mle') %>%
#                      suppressWarnings() %>% # warnings not a problem in this case
#                      tryCatch(
#                        error = function(e)   {
#                          output <<- na.result( 'no_fit_lnorm_sample',
#                                                period_label,
#                                                days)
#                          stop()
#                        }
#                      ) 
#                    
#                    a <- c(sample_fit$estimate['meanlog'], sample_fit$estimate['sdlog'])
#                    
#                  }
#                  
#                  # return values for one running window
#                  a <- as.data.frame(a)
#                  colnames(a) <- y[1] # give name to column just to suppress "new name" message
#                  
#                  a
#                }
#         )
#       
#       # bind all elements of list into a dataframe
#       a <- bind_cols(a)
#       
#       
#       
#       n_samples_possible <- ncol(a)
#       # omit columns that have NA values (didn't have enough data)
#       # use if statement to avoid changing
#       # structure of single column to vector
#       if(ncol(a)>1) a <- a[ , colSums(is.na(a)) == 0]
#       
#       # count number of sampling periods created (columns)
#       n_samples <- ncol(a)
#       
#       # calculate proportion of samples that had sufficient data
#       # for given short sampling frame length
#       n_samp_avail <- n_samples/n_samples_possible
#       
#       
#       
#       
#       
#       # apply KLD between the each short sample period and
#       # the season or entire monitoring period individually
#       
#       a <- lapply(colnames(a), function(x) {
#         
#         a <- a %>% pull(x)
#         
#         log_fit_sample <- function(x) {
#           
#           fit_value <- dlnorm(x, meanlog = a[1], sdlog = a[2])
#           
#           # make sure R does not convert to 0 if result is very small number
#           ifelse( abs(fit_value) > .Machine$double.xmin,
#                   fit_value,
#                   .Machine$double.xmin)
#         } %>%
#           tryCatch(
#             error = function(e)   {
#               output <<- na.result( 'lnorm_param_sample',
#                                     period_label,
#                                     days
#               )
#               stop()
#             }
#           )
#         
#         # find kld from two continuous functions
#         # bounded from 0 to Inf
#         kld_fit <- function(x) {
#           
#           fit_ratio_value <- (log_fit_sample(x))/(log_fit_entire(x))
#           
#           # make sure R does not convert to 0 if result is very small number
#           fit_ratio_value <- ifelse( abs(fit_ratio_value) > .Machine$double.xmin,
#                                      fit_ratio_value,
#                                      .Machine$double.xmin)
#           
#           log_fit_sample(x)*log(fit_ratio_value)
#         }
#           
#         
#         # find kld value
#         integrate(kld_fit, lower = 0,
#                   # increase tolerance to avoid "divergent" message
#                   rel.tol = .Machine$double.eps^0.5,  
#                   upper = Inf)$value %>%
#           tryCatch(
#             error = function(e)   {
#               output <<- na.result( 'integration_issue',
#                                     period_label,
#                                     days
#               )
#               stop()
#             }
#           )
#         
#       })
#       
#       
#       
#       # average all the resulting KLDs to find average KLD
#       # for specified short sampling period length
#       # and calculate the standard error
#       
#       
#       list('kld_avg' = mean(unlist(a)),
#            'kld_sd' = sd(unlist(a)),
#            
#            'n_samp_avail' = n_samp_avail)
#       
#     }
#     
#     
#     # # test kld.avg function
#     # test <- kld.avg(16)
#     
#     
#     # calculate "max KLD":
#     # average kld for short sampling periods
#     # of minimum length, "sample_days_min"
#     a <- kld.avg(sample_days_min)
#     
#     kld_avg_max <- a[['kld_avg']]
#     
#     kld_sd_max <- a[['kld_sd']]
#     
#     # apply function to find scaled_entropy dataframe for
#     # sampling period of length "days2" to range of days in tested_sample_sequence
#     entropy_data_list <- lapply(
#       tested_sample_sequence,
#       function (
#         days2 # length of short sampling period
#       ) {
#         
#         
#         # calculate average KLD for all short sampling period of length "days2"
#         a <- kld.avg(days2)
#         kld_avg <- a[['kld_avg']]
#         kld_sd <- a[['kld_sd']]
#         
#         
#         
#         # # calculate the sd for ratio of two means,
#         # # assuming independence (no covariance) between sample of given size
#         # # and minimum sample size 
#         # b <- (kld_avg/kld_avg_max)*sqrt(
#         #   kld_sd^2/kld_avg^2+
#         #     kld_sd_max^2/kld_avg_max^2
#         # )
#         
#         
#         a <- tibble(
#           'coeff' = kld_avg/kld_avg_max, # mean scaled entropy value
#           # # values for sd_int assuming kld_avg_min is a set number 
#           'high_val' = (kld_avg+kld_sd)/kld_avg_max,
#           'low_val' = (kld_avg-kld_sd)/kld_avg_max,
#           # # # values for sd_int assuming two independent samples
#           # 'high_val2' = kld_avg/kld_avg_max+b,
#           # 'low_val2' = kld_avg/kld_avg_max-b,
#           'sample_length' = days2,
#           'monitor_season' = period_label,
#           'n_samp_avail' = a[['n_samp_avail']], # amount of samples with sufficient data
#           'error' = 'none'
#         )
#   
#         a
#         
#       }
#     )
#     
#     
#     # bind all into a dataframe
#     a <- bind_rows(entropy_data_list)
#     
#     a # scaled_entropy for one date range
#     
#   } %>%
#     # if season results in an error, return the error message in a df
#     tryCatch(error = function(e) output)
#   
#   # find scaled_entropy for all short smpling lengths in all apecified time ranges
#   
#   a <- lapply(date_ranges_entire, scaled.entropy.season)
#   
#   
#   # bind all into a dataframe
#   entropy_data_season <- bind_rows(a) %>%
#     mutate(method = 'shape',
#            home = home_num,
#            metric = metric,
#            period_compare = ifelse(all_time == TRUE, 'year', 'season'))
#   
#   
#   entropy_data_season  # scaled_entropy for all specified date ranges
#   
# }
# 
#  
# 
# 
# # test function--------------------------
# start <- Sys.time()
# 
# test<- scaled.entropy.table.shape('004', 'pm25',
# 
#                            date_ranges_entire = list(
#                              c('2020-12-01', '2020-12-31'),
#                                                      c('2020-11-01',
#                                                        '2020-11-30')),
#                            tested_sample_sequence = c(4,5,6),
#                            sample_days_min = 3,
#                            all_time = TRUE)
# 
# 
# 
# end <- Sys.time()
# run2 <- end- start
# 
# # # look into bootstrapping
# # a_orig # all kld results
# # boot1 <- sample(a_orig, size = 2, replace = T)
# # boot <- lapply(1:20, function(i) sample(a_orig, replace = T))
# 

```




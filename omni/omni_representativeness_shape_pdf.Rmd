---
title: "Representativeness"
output:
  html_document:
    toc: true
    toc_float: true
    theme: paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.path = 'figures/',
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  fig.width = 10, fig.height = 3,
  cache = FALSE)
```

```{r libraries, include=FALSE}
library(dplyr)
library(purrr)
library(tidyr)
library(openair)
library(ggplot2)
library(readr)
library(googlesheets4)
library(lubridate)
library(gridExtra)
library(runner) # for moving window functions
library(ggpubr) # for tesing log-normality
# library(cubature) # for alternative method of integrating kld function
# library(entropy) # for KLD function
# library(fitdistrplus) # for fitting distribution to data NOTE, MASS::select CONFLICTS WITH dplyr::select

```

```{r import_omni_minute_data}
omni_data<- read_csv('./csv_created/omni_all_locations.csv')


zero_test <- omni_data %>%
  group_by(home, location) %>%
  summarize(pm25_zeros = sum(pm25 == 0), pm25_n = n()) %>%
  mutate(pm25_zero_pct = pm25_zeros/pm25_n*100) %>%
  ungroup()

```



```{r functions_misc}

#Function to only display 3 significant figures (for tables)
signif3 <- function(x){
  signif(x, digits = 3)
}

##Define a char vector of home numbers using a number vector,
##ex: x <- home.list(c(1:15)) for homes 1-15
threedig <- function(x) {
  if (nchar(x) == 1) {a <- paste0('00', x)
  return(a)
  }
  
  if (nchar(x) == 2) {a<- paste0('0', x)
  return(a)
  }
  else return(a)
}

home.list <- function(x) sapply(x, threedig)


#function to make density plot of variable x (a column)
dens.plot <- function(data, metric, rm) {
  
ggdensity(data %>%
            filter(room == rm) %>%
            pull(all_of(metric)), 
          main = paste("Density plot of", rm, metric),
          xlab = metric)
}




```

```{r variables}

# variables-------------------------------------------

# define seasons for date range
fall <- c('2020-09-21', '2020-12-20')
winter <- c('2020-12-21', '2021-3-20')
decemb <- c('2020-12-01', '2020-12-31')

# list of all home numbers
homes_all <- home.list(1:17)


# vector of sample lengths to make data for (when ranges are specified)
sequence <- c(3:6)

# list of seasons to use as long-monitroing period (ranges)
season_list <- list(fall)

# sequence of representative thresholds to test
threshold_seq <- c(0.1, 0.2, 0.3)

# sequence of days to test thresholds for shape entropy values

pdf_days <- c(3,7,10,14,21)


```

```{r function_pdf_shape_master}

# give a row of NA values with identifers to help with identifying
# causes of errors in function
na.result <- function(error, season = NA, sample_days = NA, samp_avail = NA) {
  tibble(
  'coeff' = NA,
  'sample_length' = sample_days, # sample_length,
  'monitor_season' = season, # period_label,
  'error' = error
)
}


# calculate kld for continuous probability distributions

# define variables for testing-----------------------
date_ranges_entire <- list(c('2020-12-02','2021-04-23'))
tested_sample_sequence <- seq(3,28,4)
sample_days_min <- 3
home_num <- '010'
location_type <- 'living'
data <- omni_data
monitor_season <- date_ranges_entire[1]
metric <- 'pm25'
days <- 16
days2 <- 16
all_time <- TRUE


date_ranges_entire <- NULL
tested_sample_sequence <- NULL
sample_days_min <- NULL
home_num <- NULL
location_type <- NULL
monitor_season <- NULL
monitor_period <- NULL
metric <- NULL
days <- NULL
days2 <- NULL
all_time <- NULL

# entropy function minutely data ----------------------------

    
# function to calculate scaled entropy for a house
# during multiple different time periods
scaled.entropy.table.shape.pdf <- function(home_num, metric,
                                       
                                       # date range of entire monitoring period
                                       # in the form: list(
                                       #                    c('YYYY-MM-DD', 'YYYY-MM-DD'),
                                       #                    c('YYYY-MM-DD', 'YYYY-MM-DD')
                                       #                  )
                                       # data in start and end day included
                                       date_ranges_entire,
                                       tested_sample_sequence, # sequence of tested short sampling period lengths (in days)
                                       location_type, # sensor location (living, bedroom, or kitchen)
                                       all_time = FALSE, # if TRUE, compare each short sampling period to entire monitoring year of home
                                       sample_days_min, # amount of days in shortest theoretical sample
                                       
                                       data = omni_data) {
  
  output <<- na.result(NA) # clear error output in case it was triggered previously
  
  if(metric %in% c('pm25', 'voc')) {
    # define LOD of metric to be 1/2 minimum detected (non-zero) value
    LOD <- data %>%
      filter(!!sym(metric)!=0) %>%
      pull(all_of(metric))%>%
      min()
  }
  
  
  # function to find scaled_entropy for samples in a given monitor season
  
  scaled.entropy.season <- function(monitor_season) {
    
    
    # must unlist the listed range that is used in lapply funnction
    monitor_season <- unlist(monitor_season)
    
    # stop if monitoring season is empty
    if(is_empty(monitor_season)) {
      output <<- na.result('likely_no_cluster')
      stop()
    } 
    

    #define label for later use in table
    period_label <- paste(as.Date(monitor_season[1]), '-',
                          as.Date(monitor_season[2]))
    
    # warn that times will be rounded to full day for entire monitoring period

    if(
        any( c(
        second(as.POSIXct(monitor_season[1])),
        minute(as.POSIXct(monitor_season[1])),
        hour(as.POSIXct(monitor_season[1])),
        
        second(as.POSIXct(monitor_season[2])),
        minute(as.POSIXct(monitor_season[2])),
        hour(as.POSIXct(monitor_season[2]))) > 0
    )
    ) {
      output <<- na.result('incorrect_date',
                           period_label)
      stop()
    }

    
    # make df of entire monitoring period (approx. a year) 
    # from all indoor sensors
    data_year <- data %>%
      filter(home == home_num & location %in% c('living',
                                                'bedroom', 'kitchen')) 
    
    if(metric %in% c('pm25', 'voc')) {
      # convert 0 values to LOD/2 in to allow for log-normal distrib. estimation
      data_year <- data_year %>%
        mutate_at(all_of(metric), function(x) ifelse(x==0, LOD/2, x))
    }
    
    # make a vector of dates when a value was recorded in year
    # in given location
    date_col_year <- data_year %>% filter(location == location_type) %>%
      pull(datetime) %>% unique()

    
    # make column of all dates (increments of 5 min) within year
    # including all locations
    all_dates_year <- seq.POSIXt(
      from = as.POSIXct( data_year%>%pull(datetime) %>% min(), tz = 'UTC'),
      
      to =  as.POSIXct( data_year%>%pull(datetime) %>% max(),
                        tz = 'UTC'),
      by = '5 min')
    
    #calculate missingness of data in year for given location
    year_data_avail <- length(date_col_year)/length(all_dates_year)
    
    
    
    # make df for season with all locations
    data_season <- data_year %>%
      # choose season date range
      filter(
        datetime >= ymd(monitor_season[1]) &
          datetime <= ymd(monitor_season[2])
      )
    
    
    # make a vector of dates when value was recorded in given location
    date_col_season <- data_season %>% filter(location == location_type) %>%
      pull(datetime) %>% unique()
    
    
    
    # make column of all dates (increments of 5 min) within season
    # including all locations
    all_dates_season <- seq.POSIXt(from = as.POSIXct( monitor_season[1], tz = 'UTC'),
                                   to =  floor_date(as.POSIXct( monitor_season[2],
                                                                tz = 'UTC')+24*60*60-1,
                                                    unit = '5 min'),
                                   by = '5 min')
    
    
    # calculate missingness of data points in the season
    season_data_avail <- length(date_col_season)/length(all_dates_season)
    

    
    if(all_time == TRUE) {
      
      # stop if missing 25% of data in year
      if(year_data_avail < 0.75) {
        output <<- na.result(
          paste0('year_missing_',
                 signif((1-year_data_avail)*100, 2),
                 '%'),
          period_label
        )
        stop()
      }
      
      entire <- data_year
      
    } else { 
      
          # stop if missing 25% of data in season
    if(season_data_avail < 0.75) {
      output <<- na.result(
        paste0('season_missing_',
               signif((1-season_data_avail)*100, 2),
               '%'),
        period_label
      )
      stop()
    }
      
      entire <- data_season
    }
    
    # make a fitted distribution for data assuming log-normal distribution
    entire_fit <- fitdistrplus::fitdist(pull(entire, metric),
                                        "lnorm", method = 'mle') %>%
      suppressWarnings() %>%
      tryCatch(
        error = function(e)   {
          output <<- na.result( 'no_fit_lnorm_entire',
                                period_label
          )
          stop()
        }
      ) 
    
    # function of continuous distribution from the parameters
    # calculated in the fitted curve
    # note that meanlog and sdlog in dlnorm correspond to the mean and sd,
    # so you do not have to take log of them yourself
    log_fit_entire <- function(x) {
      
      fit_value <- dlnorm(x, meanlog = entire_fit$estimate['meanlog'],
                          sdlog = entire_fit$estimate['sdlog'])
      
      # make sure R does not convert to 0 if result is very small number
      ifelse( abs(fit_value) > .Machine$double.xmin,
              fit_value,
              .Machine$double.xmin)
    } %>%
      tryCatch(
        error = function(e)   {
          output <<- na.result( 'lnorm_param_entire',
                                period_label
          )
          stop()
        }
      )
    
    # function to take a running window of all short sampling periods possible
    # within the longer montirong period
    kld.period <- function(days) { # days = number of days in short samping period
      
      # for testing
      # y<- all_dates_season[(159*24*12):((159+28)*24*12)]
      
      a <-
        runner(all_dates_season,
               k = days*24*12, # number of 5-min periods in short sampling period
               # only evaluate windows that start at 12AM
               # and windows that are full (ignore partial windows at start)
               at = seq(days*24*12, length(all_dates_season), 24*12),
               f = function(y) { # y = window, vector (length = k) of specified days per iteration
                 
                 # filter out only given room 
                 # and the days specified by the days in window y
                 sample <- data_season %>%
                   filter( location == location_type & datetime %in% y)
                 
                 # omit sampling period if missing more than
                 # 25 % of sampling period
                 if(nrow(sample) < 0.75 * days*24*12) {
                   
                   a <- rep(NA, 2) %>% as.integer()
                   
                   
                   
                   # if not missing 25% of sampling period...
                 } else {
                   
                   # fit a lognormal distribution to sample and extract
                   # logmean and logsd values
                   sample_fit <- fitdistrplus::fitdist(pull(sample, metric),
                                                       "lnorm", method = 'mle') %>%
                     suppressWarnings() %>% # warnings not a problem in this case
                     tryCatch(
                       error = function(e)   {
                         output <<- na.result( 'no_fit_lnorm_sample',
                                               period_label,
                                               days)
                         stop()
                       }
                     ) 
                   
                   a <- c(sample_fit$estimate['meanlog'], sample_fit$estimate['sdlog'])
                   
                 }
                 
                 # return values for one running window
                 a <- as.data.frame(a)
                 colnames(a) <- y[1] # give name to column just to suppress "new name" message
                 
                 a
               }
        )
      
      # bind all elements of list into a dataframe
      a <- bind_cols(a)
      
      
            # omit columns that have NA values (didn't have enough data)
      # use if statement to avoid changing
      # structure of single column to vector
      if(ncol(a)>1) a <- a[ , colSums(is.na(a)) == 0]
      
      # count number of sampling periods created (columns)
      n_samples <- ncol(a)
      
      
      # apply KLD between the each short sample period and
      # the season or entire monitoring period individually
      
      a <- lapply(colnames(a), function(x) {
        
        a <- a %>% pull(x)
        
        log_fit_sample <- function(x) {
          
          fit_value <- dlnorm(x, meanlog = a[1], sdlog = a[2])
          
          # make sure R does not convert to 0 if result is very small number
          ifelse( abs(fit_value) > .Machine$double.xmin,
                  fit_value,
                  .Machine$double.xmin)
        } %>%
          tryCatch(
            error = function(e)   {
              output <<- na.result( 'lnorm_param_sample',
                                    period_label,
                                    days
              )
              stop()
            }
          )
        
        # find kld from two continuous functions
        # bounded from 0 to Inf
        kld_fit <- function(x) {
          
          fit_ratio_value <- (log_fit_sample(x))/(log_fit_entire(x))
          
          # make sure R does not convert to 0 if result is very small number
          fit_ratio_value <- ifelse( abs(fit_ratio_value) > .Machine$double.xmin,
                                     fit_ratio_value,
                                     .Machine$double.xmin)
          
          log_fit_sample(x)*log(fit_ratio_value)
        }
          
        
        # find kld value
        integrate(kld_fit, lower = 0,
                  # increase tolerance to avoid "divergent" message
                  rel.tol = .Machine$double.eps^0.5,  
                  upper = Inf)$value %>%
          tryCatch(
            error = function(e)   {
              output <<- na.result( 'integration_issue',
                                    period_label,
                                    days
              )
              stop()
            }
          )
        
      })
      
a # return all calculated kld values

    }
    # # test kld.period function
    # test <- kld.period(16)
    
    # calculate "max KLD":
    # average kld for smallest sampling length of interest
    kld_avg_max <- mean(kld.period(sample_days_min) %>% unlist(), na.rm =TRUE)
    
    

    # apply function to find scaled_entropy dataframe for
    # sampling period of length "days2" to range of days in tested_sample_sequence
    entropy_data_list <- lapply(tested_sample_sequence,
                                function (
                                  days2 # length of short sampling period
                                ) {
                                  
                                  
                                  # calculate  KLD for all
                                  # short sampling period of length "days2"
                                  a <- kld.period(days2) %>% unlist()
                                  
                                  
                                  
                                  
                                  a <- tibble(
                                    'coeff' = a/kld_avg_max, # mean scaled entropy value
                                    'sample_length' = days2,
                                    'monitor_season' = period_label,
                                    'error' = 'none'
                                  )
                                  
                                  
                                  a
                                  
                                }
    )
    
    
    # bind all into a dataframe
    a <- bind_rows(entropy_data_list)
    
    a # return all scaled_entropy values for one date range
    
  } %>%
    # if season results in an error, return the error message in a df
    tryCatch(error = function(e) output)
  
  # find scaled_entropy for all short smpling lengths in all apecified time ranges
  
  a <- lapply(date_ranges_entire, scaled.entropy.season)
  
  
  # bind all into a dataframe
  entropy_data_season <- bind_rows(a) %>%
    mutate(method = 'shape',
           home = home_num,
           metric = metric,
           location = location_type,
           period_compare = ifelse(all_time == TRUE, 'year', 'season'))
  
  
  entropy_data_season  # scaled_entropy for all specified date ranges
  
}

 


# test function--------------------------
start <- Sys.time()

test<- scaled.entropy.table.shape.pdf('004', 'pm25',

                           date_ranges_entire = list(
                             c('2020-12-01', '2020-12-31'),
                                                     c('2020-11-01',
                                                       '2020-11-30')),
                           tested_sample_sequence = c(4,5,6),
                           location_type = 'living',
                           sample_days_min = 3,
                           all_time = TRUE)




end <- Sys.time()
run2 <- end- start

```






## Distribution Shape: Assume Log-normal distribution



```{r data_maker, eval = FALSE}
# import energy cluster dataframe for all homes
energy_cluster_df <- read_csv('../sense/csv_created_sense/energy_cluster_df.csv')

# import summary df of how many lags required
# before autocorrelation was insignificant
lag_summary_df <- read_csv('./csv_created/lag_summary.csv')

# make functions to look up starting and ending date of
# energy cluster period based on home
 pick.start <- function(x_home, x_cluster) {
   energy_cluster_df %>%
   filter(home == x_home & cluster_type == x_cluster) %>%
   pull(start_date) %>% as.character()
 }
 
  pick.end <- function(x_home, x_cluster) {
   energy_cluster_df %>%
   filter(home == x_home & cluster_type == x_cluster) %>%
   pull(end_date) %>% as.character()
  }
  
  # fun to pick minimum sample length based on metric
  pick.sample.min <- function(x_metric) {
    lag_summary_df %>%
      # use the pooled median of all clusters
      filter(metric == x_metric, energy_cluster == 'total') %>%
      pull(median)
  }
  # # test functions
   # pick.start('006', 'heat')
  # pick.end('004', 'heat')


  all_time_choice <- TRUE
 
  # pm25------------------
   met <- 'pm25'
 
   loc <- 'living'
   
      clust <- 'shoulder'





# test function for multiple homes
test_pdf <- lapply(homes_all, function(x_home) {
  
  start_date <- pick.start(x_home, clust)
  end_date <- pick.end(x_home, clust)
  sample_min <- pick.sample.min(met)
  sample_max <- if_else(as.numeric(as.Date(end_date) - as.Date(start_date))>=28,
                        28, as.numeric(as.Date(end_date) - as.Date(start_date)))
  
  a <- 
    scaled.entropy.table.shape.pdf(x_home, metric = met,
                                           date_ranges_entire = list(c(start_date,
                                                                       end_date)),
                                           tested_sample_sequence =
                                             seq(sample_min,sample_max,8),
                                           location_type = loc,
                                           sample_days_min = sample_min,
                                           all_time = all_time_choice) %>%
                  mutate(energy_cluster = clust) 
  
    # give a variable name to the created data
    assign(paste(met, loc, clust, 'rep_data_shape_pdf', sep = '_'),
           rep_data,
           envir = .GlobalEnv)
    
  a
}
) %>%
  bind_rows()





# make csvs--------------------

# Notes on run time:
# if doing 2 seasons
# takes about 1.4 minutes per sample_length per home (assume 2 if full season)



rep_data_all <- bind_rows(pm25_bedroom_heat_rep_data_shape_pdf,
                       pm25_living_heat_rep_data_shape_pdf,
                       pm25_kitchen_heat_rep_data_shape_pdf,
                       pm25_bedroom_shoulder_rep_data_shape_pdf,
                       pm25_living_shoulder_rep_data_shape_pdf,
                       pm25_kitchen_shoulder_rep_data_shape_pdf
                       pm25_bedroom_ac_rep_data_shape_pdf,
                       pm25_living_ac_rep_data_shape_pdf,
                       pm25_kitchen_ac_rep_data_shape_pdf)

# make csv of all season data
write_csv(rep_data_all, file =
            paste0('./csv_created/representativeness_data/rep_data_shape_pdf_', Sys.Date(), '.csv'))



# read in csv file
entropy_shape_season_pm25 <- read_csv(file = './csv_created/representativeness_data/rep_data_shape_pdf.csv')


# # for binding new rows to old rows
# a1 <- read_csv(file = './representativeness_data/rep_data_shape_season_pm25_ihs_2021-04-01.csv')
# 
# a <- rbind(a1, entropy_shape_season_pm25)
# 
# write_csv(a, file =
#             paste0('./representativeness_data/rep_data_shape_season_', met, '_', Sys.Date(), '.csv'))



# for one home:
a <- entropy_shape_season_pm25 %>%
  filter(home == '009') %>%
  filter(
         monitor_period == paste(as.Date(fall[1]), '-',
                             as.Date(fall[2]))

|
         monitor_period == paste(as.Date(winter[1]), '-',
                             as.Date(winter[2]))

|
         monitor_period == paste(as.Date(decemb[1]), '-',
                             as.Date(decemb[2])) |
         monitor_period == 'Dec'


)

# plot results
ggplot(aes(x = sample_length, y = coeff, color = monitor_period),
       data = a)+
  geom_line()+
  geom_point(size = 1)+
  xlab('Sample Length, days')+
  ylab('Scaled Relative Entropy')+
  geom_hline(yintercept=0.1, linetype="dashed", 
                color = "black", size=0.5)+
  coord_cartesian(ylim = c(0,0.5))




```


```{r shape_data_maker_range_pdf, eval = FALSE}



# Notes on run time:
# if doing 2 seasons
# takes about 1.4 minutes per sample_length per home (assume 2 if full season)

# pm25_ihs--------------------------------------
met <- 'pm25_ihs'

home1 <- make.data.range.pdf.shape('001', met, season_list, sequence)
home2 <- make.data.range.pdf.shape('002', met, season_list, sequence)

start <- Sys.time()
home3 <- make.data.range.pdf.shape('003', met, season_list, sequence)
end <- Sys.time()
time <- end-start

home4 <- make.data.range.pdf.shape('004', met, season_list, sequence)
home5 <- make.data.range.pdf.shape('005', met, season_list, sequence)
home6 <- make.data.range.pdf.shape('006', met, season_list, sequence)
home7 <- make.data.range.pdf.shape('007', met, season_list, sequence)
home8 <- make.data.range.pdf.shape('008', met, season_list, sequence)
home9 <- make.data.range.pdf.shape('009', met, season_list, sequence)
home10 <- make.data.range.pdf.shape('010', met, season_list, sequence)
home11 <- make.data.range.pdf.shape('011', met, season_list, sequence)
home12 <- make.data.range.pdf.shape('012', met, season_list, sequence)
home13 <- make.data.range.pdf.shape('013', met, season_list, sequence)
home14 <- make.data.range.pdf.shape('014', met, season_list, sequence)
home15 <- make.data.range.pdf.shape('015', met, season_list, sequence)
home16 <- make.data.range.pdf.shape('016', met, season_list, sequence)


# make a list of all dataframes
home_data_list <- lapply(c(1:16),
                        function(x) tryCatch(get(paste0('home', x)),
                                             error = function(e){
                                               NULL
                                               }
                                             ))



# #combine into one dataframe
# home_data3 <- do.call(rbind, home_data_list)
# 
# 
# # make csv of all season data
# write_csv(rbind(home_data, home_data2, home_data3), file =
#             paste0('./representativeness_data/rep_data_shape_season_pdf_3_', met, '_', Sys.Date(), '.csv'))


#combine into one dataframe
home_data <- do.call(rbind, home_data_list)


# to combine new data with old data
# home_data <- do.call(home_data, entropy_shape_season_pdf_pm25)


# make csv of all season data
write_csv(home_data, file =
            paste0('./representativeness_data/rep_data_shape_season_pdf_', met, '_', Sys.Date(), '.csv'))




# # for binding new rows to old rows
# a1 <- read_csv(file = './representativeness_data/rep_data_shape_season_pdf_pm25_ihs_2021-04-01.csv')
# 
# a <- rbind(a1, entropy_shape_season_pm25)
# 
# write_csv(a, file =
#             paste0('./representativeness_data/rep_data_shape_season_pdf_', met, '_', Sys.Date(), '.csv'))






```

### Testing Different Entropy Thresholds

### Effect on when average Scaled Entropy crosses below threshold

```{r threshold_plots_shape}

# months--------------------------------------------------------------

# # make dataframe of values for minimum sample length (in days) where entropy is less than 0.1
# entropy_cutoffs <- entropy_all %>%
#   filter(n_samp_avail >0.5 & method == 'shape') %>%
#   filter(
#     monitor_period %in% month.abb
#     ) %>%
#   filter(coeff <= threshold) %>% 
#   group_by(home, metric, monitor_period) %>%
#   summarise_at(vars(sample_length), min) %>%
#   ungroup()
# 
# # fill in missing combinations so bar chart will have bars of constant width
# a <- expand.grid(home=unique(entropy_cutoffs$home),
#                  metric=unique(entropy_cutoffs$metric),
#                  monitor_period=unique(entropy_cutoffs$monitor_period))%>%
#   data.frame() %>%
#   left_join(entropy_cutoffs)
# 
# 
# 
# # bar chart of when threshold was reached for each home
# ggplot(aes(x = home, y = sample_length, fill = metric), data = a)+
#   geom_col(position = 'dodge')+
#   facet_wrap(vars(monitor_period), nrow = 2)
# 
# 
# 
# 
# 
# 
# 
# # make dataframe of values for minimum sample length (in days)
# # where entropy is less than threshold value
# threshold.freq.table <- function (threshold) {
#   
#   
#   entropy_cutoffs <- entropy_all %>%
#   filter(n_samp_avail >0.5 & method == 'shape') %>%
#   filter(
#     monitor_period %in% month.abb
#     ) %>%
#   filter(coeff <= threshold) %>% 
#   group_by(home, metric, monitor_period) %>%
#   summarise_at(vars(sample_length), min) %>%
#   ungroup()
#     
# 
#   # fill in missing combinations so bar chart will have bars of constant width
# a <- expand.grid(home=unique(entropy_cutoffs$home),
#                  metric=unique(entropy_cutoffs$metric),
#                  monitor_period=unique(entropy_cutoffs$monitor_period))%>%
#   data.frame() %>%
#   left_join(entropy_cutoffs) %>%
# # count frequency of home-month instances when threshold was reached for each sample length
#   group_by(sample_length, metric) %>%
#   summarise(freq = n()) %>%
#   ungroup() %>%
#   filter(!is.na(sample_length)) %>%
#   mutate(thresh = as.factor(threshold))
# }
# 
# 
# # apply function to all thresholds in threshold_seq
#  a <- lapply(threshold_seq, threshold.freq.table)
#  a <- do.call(rbind, a)
# 
#  # ensure there weren't a significant amount of omissions for any metric type
#  # (because the threshod may never have been reached)
#  
#  # test <- a %>%
#  #   group_by(metric) %>%
#  #   summarise(n = sum(freq)) %>%
#  #   ungroup()
# 
#  # histogram of when threshold was reached for all home/month combos
# ggplot(aes(x = sample_length, fill = thresh), data = a)+
#   geom_density(alpha = 0.5)+
#   facet_wrap(vars(metric))+
#   xlab("Sample Length Required to Meet Representativeness Threshold, days")+
#   scale_x_continuous(breaks = c(7,14,21,28))


# maybe limit to Aug-Feb (7 months), 15 homes (4 metrics) for now?


# seasons--------------------------------------------------------------


# make dataframe of values for minimum sample length (in days)
# where entropy is less than threshold value
threshold.freq.table.seasons <- function (threshold) {
  
  
  a <- entropy_all %>%
  filter(n_samp_avail >0.5 & method == 'shape') %>%
  filter(
    monitor_period %in% c('fall', 'winter')
    ) %>%
  filter(coeff <= threshold) %>% 
  group_by(home, metric, monitor_period) %>%
  summarise_at(vars(sample_length), min) %>%
  ungroup() %>%
    mutate('thresh' = threshold)
    

}


# apply function to all thresholds in threshold_seq
 a <- lapply(threshold_seq, threshold.freq.table.seasons)
 a <- do.call(rbind, a)

 # ensure there weren't a significant amount of omissions for any metric type
 # (because the threshod may never have been reached)
 
 # test <- a %>%
 #   group_by(metric) %>%
 #   summarise(n = sum(freq)) %>%
 #   ungroup()
 
   lvls <- levels(as.factor(a$thresh))
rows <- by(a, a$thresh, function(x) nrow(x))
labels <- paste(lvls,", n=",as.integer(rows),sep="")

 # histogram of when threshold was reached for all home/month combos
ggplot(aes(x = sample_length, fill = as.factor(thresh)), data = a)+
  geom_density(alpha = 0.5)+
  facet_wrap(vars(metric))+
  xlab("Sample Length Required to Meet Representativeness Threshold, days")+
  scale_x_continuous(breaks = c(7,14,21,28))+
  scale_fill_discrete(labels = labels)+
  annotate('text', x = 14, y = 0.15, label = '*max n = 16 (number of homes)')



```

### Effect on when individual sample Scaled Entropy crosses below threshold

```{r pdf_plots_shape}

# read in csv file
entropy_shape_season_pdf_pm25 <- read_csv(file = './representativeness_data/rep_data_shape_season_pdf_pm25_ihs.csv')


entropy_all_pdf_shape <- entropy_shape_season_pdf_pm25  %>%
  # ensure home column is not numeric
  mutate_at(vars(home), function(x) {
    if(is.numeric(x)) home.list(x) else x
    }
    ) %>%
  mutate(
      # make a column specifying shoulder seasons
    season = case_when(
      monitor_period %in% month.abb[c(10, 4)] ~ 'shoulder',
          monitor_period %in% month.abb[c(11:12,1:3, 5:9)] ~ 'non-shoulder'),
    # convert time periods to seasons
    monitor_period = case_when(
      monitor_period == paste(as.POSIXct(fall[1]),
                              as.POSIXct(fall[2]), sep = ' - ') ~ 'fall',
            monitor_period == paste(as.POSIXct(winter[1]),
                              as.POSIXct(winter[2]), sep = ' - ') ~ 'winter',
                  monitor_period == paste(as.POSIXct(decemb[1]),
                              as.POSIXct(decemb[2]), sep = ' - ') ~ 'decemb',
      TRUE ~ monitor_period

    )
  )


# seasons pdf---------------------------------------------------------------


a <- entropy_all_pdf_shape %>%
  filter(sample_length %in% pdf_days)

# # count the fraction of 3-day samples that have a scaled entropy > 1
# (entropy_all_pdf_shape %>%
#   filter(sample_length == 3, coeff >1) %>%
#   nrow())/
#   (entropy_all_pdf_shape %>%
#   filter(sample_length == 3) %>%
#   nrow())


lvls <- levels(as.factor(a$sample_length))
non_na <- by(a, a$sample_length, function(x) sum(!is.na(x$coeff)))
above1 <- by(a, a$sample_length, function(x) sum(x$coeff >1, na.rm = TRUE))
labels <- paste(lvls,", n=",as.integer(non_na),", ", above1, " values > 1",sep="")




thresh_vector <- c(0.1, 0.2, 0.3)

ggplot(a, aes(x = coeff, fill = as.factor(sample_length)))+
    geom_density(alpha = 0.5)+
  xlab("Scaled Entropy")+
    ggtitle(label = 'Scaled Entropy at multiple Sample Lengths, days')+
  coord_cartesian(xlim = c(0,1))+ # limit scaled entropy from 0 to 1
  labs(fill = 'Length of Sample, days')+
  geom_vline(xintercept = thresh_vector, linetype = 'dashed', color = 'red')+
  annotate('text', x=thresh_vector[1]-0.02, y = -0.1,
                label=thresh_vector[1],
            angle=90, size=3, color = 'red') +
   annotate('text', x=thresh_vector[2]-0.02, y = -0.1,
                label=thresh_vector[2],
            angle=90, size=3, color = 'red') +
    annotate('text', x=thresh_vector[3]-0.02, y = -0.1,
                label=thresh_vector[3],
            angle=90, size=3, color = 'red')+
     scale_fill_discrete( labels=labels)





```




---
title: "Time - Representativeness"
output:
  html_document:
    toc: true
    toc_float: true
    theme: paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.path = 'figures/',
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  fig.width = 10, fig.height = 3,
  cache = FALSE)

```

```{r libraries, include=FALSE}
library(dplyr)
library(purrr)
library(tidyr)
library(openair)
library(ggplot2)
library(readr)
library(googlesheets4)
library(lubridate)
library(gridExtra)
library(runner) # for moving window functions
library(ggpubr) # for tesing log-normality
# library(cubature) # for alternative method of integrating kld function
# library(entropy) # for KLD function
# library(fitdistrplus) # for fitting distribution to data NOTE, MASS::select CONFLICTS WITH dplyr::select

```

```{r import_omni_minute_data}
# omni_data<- read_csv('./csv_created/omni_all_locations.csv')

# # remove all variables but omni_data
# rm(list=setdiff(ls(), "omni_data"))

# zero_test <- omni_data %>%
#   group_by(home, location) %>%
#   summarize(pm25_zeros = sum(pm25 == 0), pm25_n = n()) %>%
#   mutate(pm25_zero_pct = pm25_zeros/pm25_n*100) %>%
#   ungroup()

```

```{r import_omni_hourly}
omni_hourly<- read_rds('./csv_created/omni_hourly_calibrated.rds')

```

```{r censor_date}
omni_hourly <- omni_hourly %>% filter(datehour<ymd_hms('2021-06-01 00:00:00'))


```

```{r import_energy_cluster_homes}
# import energy cluster dataframe for all homes
energy_cluster_df <- read_csv('../sense/csv_created_sense/energy_cluster_df.csv')
```

```{r import_acf_lags}
# import summary df of how many lags required
# before autocorrelation was insignificant
lag_summary_df <- read_csv('./csv_created/lag_summary.csv')
```



```{r functions_misc}

#Function to only display 3 significant figures (for tables)
signif3 <- function(x){
  signif(x, digits = 3)
}

##Define a char vector of home numbers using a number vector,
##ex: x <- home.list(c(1:15)) for homes 1-15
threedig <- function(x) {
  if (nchar(x) == 1) {a <- paste0('00', x)
  return(a)
  }
  
  if (nchar(x) == 2) {a<- paste0('0', x)
  return(a)
  }
  else return(a)
}

home.list <- function(x) sapply(x, threedig)


# function to calculate Kullback-Liebler Distance between two vectors
# of equal length
kld <- function(p,q){
  lapply(1:length(p), function(x){
         p <- p[x]/sum(p)
         q <- q[x]/sum(q)
         
         p*log(p/q)
         }
         ) %>% unlist() %>% sum()
}

# give a row of NA values with identifers to help with identifying
# causes of errors in function
na.result <- function(error, season = NA, sample_days = NA, samp_avail = NA) {
  tibble(
  'kld' = NA,
  'sample_length' = sample_days, # sample_length,
  'monitor_season' = season, # period_label,
  'n_samp_avail' = samp_avail,
  'error' = error,
  'warn' = warn # return wrnaing if there was one triggered

)
}

```


```{r define_variables}

# list of all home numbers
homes_all <- home.list(1:16)
clusters_all <- c('heat','shoulder', 'ac'
                  # 'all'
                  )
locations_indoor <- c('living', 'kitchen', 'bedroom')

```



```{r function_pdf_time_hourly_master}


# calculate kld for continuous probability distributions

# define variables for testing-----------------------

# date_ranges_entire <- list(c('2020-08-18','2020-09-13'))
# tested_sample_sequence <- seq(1,28)
# home_num <- '007'
# location_type <- 'living'
# data <- omni_hourly
# monitor_season <- date_ranges_entire[1]
# metric <- 'pm25'
# days <- 27
# days2 <- 27
# all_time <- TRUE
#     longterm_avail_limit <- 0
#     sample_avail_limit <- 0
#     overlap <- FALSE
#     
# 
# 
# # remove variables after done testing
# rm(tested_sample_sequence,
#    date_ranges_entire,
# sample_days_min,
# home_num,
# monitor_season,
# monitor_period,
# metric,
# days,
# days2,
# all_time,
# data,
# data_season,
# data_year,
# entire,
# date_col_season,
# date_col_year,
# all_dates_season,
# all_dates_year
# )

# # test runner function
# test <- data.frame(x = c(1:30), y = 100*c(1:30))
# test2<- runner(test$y,
#        k = 16,
#        at = seq(16,30,16),
#        f = function(t) t)

# entropy function hourly data ----------------------------

    
# function to calculate scaled entropy for a house
# during multiple different time periods
entropy.table.hourly <- function(
  home_num, metric,
  
  # date range of entire monitoring period
  # in the form: list(
  #                    c('YYYY-MM-DD', 'YYYY-MM-DD'),
  #                    c('YYYY-MM-DD', 'YYYY-MM-DD')
  #                  )
  # data in start and end day included
  date_ranges_entire,
  tested_sample_sequence, # sequence of tested short sampling period lengths (in days)
  location_type, # sensor location (living, bedroom, kitchen, or pooled)
  overlap, # if FALSE, only use samples made of ditinct days
  all_time, # if TRUE, compare each short sampling period to entire monitoring year of home
    longterm_avail_limit = 0, # only use data from a long-term period if it has at least this fraction of data available (0 means no periods are omitted if they have any data)
  sample_avail_limit = 0, # only use data from a sample period if it has at least this fraction of data available (0 means no periods are omitted if they have any data)
  data = omni_hourly) {
  
      warn <<- 'none'# clear warning output in case it was triggered previously
  output <<- na.result(NA) # clear error output in case it was triggered previously
  
  if(metric %in% c('pm25', 'voc')) {
    # define LOD of metric to be 1/2 minimum detected (non-zero) value
    LOD <- data %>%
      filter(!!sym(metric)!=0) %>%
      pull(all_of(metric))%>%
      min()
  }
  
  
  # function to find scaled_entropy for samples in a given monitor season
  
  scaled.entropy.season <- function(monitor_season) {
    
    
    # must unlist the listed range that is used in lapply funnction
    monitor_season <- unlist(monitor_season)
    
    # stop if monitoring season is empty
    if(is_empty(monitor_season)) {
      output <<- na.result('likely_no_cluster')
      stop()
    }
    
    
    #define label for later use in table
    period_label <- paste(as.Date(monitor_season[1]), '-',
                          as.Date(monitor_season[2]))
    
    # warn that times will be rounded to full day for entire monitoring period
    
    if(
      any( c(
        second(as.POSIXct(monitor_season[1])),
        minute(as.POSIXct(monitor_season[1])),
        hour(as.POSIXct(monitor_season[1])),
        
        second(as.POSIXct(monitor_season[2])),
        minute(as.POSIXct(monitor_season[2])),
        hour(as.POSIXct(monitor_season[2]))) > 0
      )
    ) {
      output <<- na.result('incorrect_date',
                           period_label)
      stop()
    }

    
    # make df of entire monitoring period (approx. a year)
    # recorded from given location
    data_year <- data %>%
      filter(home == home_num) %>%
      arrange(datehour)
    
    data_year <-
      if(location_type == 'pooled') {
        data_year %>% filter(location %in% locations_indoor)
      } else{
        data_year %>% filter(location == location_type)
      }
    
    data_year <- data_year %>%
      mutate(hour = hour(datehour)
             # datehour = floor_date(datehour, unit = 'hour')
             )
    
    
    
    if(metric %in% c('pm25', 'voc')) {
      # convert 0 values to LOD/2 in to allow for log-normal distrib. estimation
      data_year <- data_year %>%
        mutate_at(all_of(metric), function(x) ifelse(x==0, LOD/2, x))
    }
    
    
    # make column of all dates (increments of 1 hour) within sampling year
    # including all locations
    all_dates_year <- seq.POSIXt(
      from = as.POSIXct(
        data %>%
          filter(home == home_num &
                   location %in% c('living', 'bedroom', 'kitchen'))%>%
          pull(datehour) %>% min(), tz = 'UTC'),
      
      to =  as.POSIXct( data %>%
                          filter(home == home_num &
                                   location %in% c('living', 'bedroom', 'kitchen'))%>%
                          pull(datehour) %>% max(), tz = 'UTC'),
      by = 'hour')
    
    
    #calculate missingness of data in year for given location
    year_data_avail <-
      length(data_year %>% pull(datehour) %>% unique())/
      length(all_dates_year)
    
    
    
    
    # make df for recorded values in season for given location
    data_season <- data_year %>%
      # choose season date range
      filter(
        datehour >= ymd(monitor_season[1]) &
          datehour <= ymd(monitor_season[2])
      )
    
    # make column of all dates (increments of 1 hour) within season
    all_dates_season <- seq.POSIXt(from = as.POSIXct( monitor_season[1], tz = 'UTC'),
                                   to =  floor_date(as.POSIXct( monitor_season[2],
                                                                tz = 'UTC')+24*60*60-1,
                                                    unit = 'hour'),
                                   by = 'hour')

    # calculate missingness of data points in the season
    season_data_avail <-
      length(data_season %>% pull(datehour) %>% unique())/
      length(all_dates_season)
    
    if(all_time == TRUE) {
      
      # stop if missing 25% of data in year
      if(year_data_avail < longterm_avail_limit) {
        output <<- na.result(
          paste0('year_missing_',
                 signif((1-year_data_avail)*100, 2),
                 '%'),
          period_label
        )
        stop()
      }
      
      entire <- data_year
      
    } else {
      
      # stop if missing 25% of data in season
      if(season_data_avail < longterm_avail_limit) {
        output <<- na.result(
          paste0('season_missing_',
                 signif((1-season_data_avail)*100, 2),
                 '%'),
          period_label
        )
        stop()
      }
      
      entire <- data_season
    }
    
    entire <- entire %>%
      group_by(hour) %>%
  summarise(mean = mean(get(metric)), sd = sd(get(metric)), n = n(),
            .groups = 'drop') %>%
      arrange(hour)
    
    entire_hourly_n <- entire %>% pull(n)
    entire_hourly_sd <- entire %>% pull(sd)
    entire <- entire %>% pull(mean)
    # ensure no na values
    if(any(is.na(entire))) {
      output <<- na.result('na_in_entire', period_label)
      stop()
    }
    
    
    
    # stop if an hour period of day is completely missing data
    if(length(entire) < 24) {
      output <<- na.result('missing_hour', period_label)
      stop()
    }

    # find max and where it occurs
    max_entire <- max(entire)
            max_hour_entire <- which(entire==max(entire))
        
            # stop if max occurs at multiple hours
    if(length(max_hour_entire) >1) {
      output <<- na.result(paste0(length(max_hour_entire),'_max_hours'), period_label)
      stop()
    }
        
    
    # function to take a running window of all short sampling periods possible
    # within the longer montirong period,
    
    kld.period <- function(days) { # days = number of days in short samping period
      
      # for testing
      # y<- all_dates_season[(1*25):((1+15)*24)]

      # warn if not enough days in season to calculate for given sampling period length
      if(length(all_dates_season) <= days*24) {
        warn <<- 'short_season'
        
              # a <- list(
              #   'sample_means' = NA %>% as.integer(),
              #   'sample_hourly_n' = NA %>% as.integer(),
              #   'sample_hourly_sd' = NA %>% as.integer(),
              #   'sample_period' = NA%>% as.integer()
              # 
              # ) 

              a <- tibble(
                max_entire = max_entire,
               max_hour_entire = max_hour_entire,
               entire_hourly_n = list(entire_hourly_n),
               entire_hourly_sd = list(entire_hourly_sd),
               entire_data = list(entire)
              )
  
                      
      } else {
        
      a <-
        runner(
          all_dates_season %>% as.character(),
          k = days*24, 
          # number of hour periods in short sampling period
               # only evaluate windows that are full
               # (ignore partial windows at start)
               # only use disticnt days if overlap == FALSE
               at = seq(days*24, length(all_dates_season),
                        {if(overlap == TRUE) 24 else days*24}),
          f = function(y) { # y = window, vector (length = k) of specified days per iteration
            
            # filter out only given room
            # and the days specified by the days in window y
            sample <- data_season %>%
              filter(
                (datehour %>% as.character()) %in% y
              )
            
            
            
            sample <- 
              if(location_type == 'pooled') {
                sample %>% filter(location %in% locations_indoor)
              } else{
                sample %>% filter(location == location_type)
              }
            
            sample_period <- paste(
                y[1] %>% ymd_hms() %>% floor_date( unit = 'day') %>% as.character(),
                '-',
                y[length(y)] %>% ymd_hms() %>%
                  floor_date( unit = 'day') %>% as.character()
              )
            
            # omit sampling period if missing more than
            # __ % of sampling period
            if(length(sample$datehour %>%unique()) < sample_avail_limit * days*24) {
              
              sample_results <- list(
                'sample_means' = rep(NA, 24) %>% as.integer(),
                'sample_hourly_n' = rep(NA, 24) %>% as.integer(),
                'sample_hourly_sd' = rep(NA, 24) %>% as.integer(),
                'sample_period' = sample_period

              )
              
              
              # if not missing certain % of sampling period...
            } else {
              
              sample <- sample %>%
                group_by(hour) %>%
                summarise(mean = mean(get(metric)), sd = sd(get(metric)), n = n(),
            .groups = 'drop')%>%
      arrange(hour)
              
              sample_hourly_n <- sample %>% pull(n)
              sample_hourly_sd <- sample %>% pull(sd)
              
              sample <- sample %>% pull(mean)
              
              sample_results <- list(
                'sample_means' = sample, 'sample_hourly_n' = sample_hourly_n,
                'sample_hourly_sd' = sample_hourly_sd,
                'sample_period' = sample_period
              )
              
              # ensure no na values
              if(any(is.na(sample_results[['sample_means']]))) {
                output <<- na.result('na_in_sample', period_label)
                stop()
              }
              
              # omit sampling period if there is not an avg value
              # for all hour bins in the period
              if(length(sample_results[['sample_means']])<24) {
              sample_results <- list(
                'sample_means' = rep(NA, 24) %>% as.integer(),
                'sample_hourly_n' = rep(NA, 24) %>% as.integer(),
                'sample_hourly_sd' = rep(NA, 24) %>% as.integer(),
                'sample_period' = sample_period

              )              }
              
            }  

sample_results
          }
        )
      

     
      
      a <- as.data.frame(a)
      
      
      a <- map(colnames(a), function(x) {
        tibble('sample_period' = a[[x]][['sample_period']],
               'sample_means' = a[[x]]['sample_means'],
               'sample_hourly_n' = a[[x]]['sample_hourly_n'],
               'sample_hourly_sd' = a[[x]]['sample_hourly_sd'],
        )
      })
      
      n_samples_possible <- length(a)
      # omit sample results from samples that have NA means (didn't have enough data)
      
      a <- a[
        !map(a, function(x){ any(is.na(x[['sample_means']][['sample_means']]))
        }
        ) %>% unlist()
      ]
      
      
      
      
      # count number of sampling periods created (columns)
      n_samples <- length(a)
      
      # calculate proportion of samples that had sufficient data
      # for given short sampling frame length
      n_samp_avail <- n_samples/n_samples_possible
      a <- bind_rows(a)
      

      
      # # stop if max occurs at multiple hours
      # if(length(max_hour_sample) >1) {
      #   output <<- na.result(paste0(length(max_hour_sample),
      #                               '_max_hours'), period_label, days)
      #   stop()
      # }

              # return all calculated kld values
      
      a <- a %>%
        rowwise()%>%
        mutate(
          kld = kld(sample_means, entire),
          max_sample = max(sample_means),
          max_hour_sample = ifelse(!is.na(max_sample),
                                   which(sample_means ==max_sample), NA))
      a<- a %>%
        mutate(n_samp_avail = n_samp_avail,
               max_entire = max_entire,
               max_hour_entire = max_hour_entire,
               entire_hourly_n = list(entire_hourly_n),
               entire_hourly_sd = list(entire_hourly_sd),
               entire_data = list(entire)) %>%
        rename(sample_data = sample_means)
      

      }      

    a  
    }
    # # test kld.period function
    # test <- kld.period(28)

            # apply function to find scaled_entropy dataframe for
    # sampling period of length "days2" to range of days in tested_sample_sequence
    entropy_data_list <- lapply(
      tested_sample_sequence,
      function (
        days2 # length of short sampling period
      ) {
        
        
        # calculate  KLD for all
        # short sampling period of length "days2"
        a <- kld.period(days2)

        a %>% mutate(
          sample_length = days2,
          monitor_season = period_label,
          error = 'none',
          warn = warn
        )
        
        
      }
    )
    
    
    # bind all into a dataframe scaled_entropy for one date range
   bind_rows(entropy_data_list)
    
  } %>%
    # if season results in an error, return the error message in a df
    tryCatch(error = function(e) output)
  
  # find scaled_entropy for all short smpling lengths in all apecified time ranges
  
  a <- lapply(date_ranges_entire, scaled.entropy.season)
  
  
  # bind all into a dataframe
  entropy_data_season <- bind_rows(a) %>%
    mutate(method = 'time',
           home = home_num,
           metric = metric,
           location = location_type,
           period_compare = ifelse(all_time == TRUE, 'year', 'season'),
                      overlap = overlap
           )
  
  
  entropy_data_season  # scaled_entropy for all specified date ranges
  
}

 

# test function--------------------------

# start <- Sys.time()
# # 
# test<- entropy.table.hourly('007', 'pm25',
#                             overlap = FALSE,
#                            date_ranges_entire = list(
#                              c('2020-08-18', '2020-09-13')),
#                            tested_sample_sequence = c(1,28),
#                            location_type = 'living',
#                            all_time = TRUE)
# 
# 
# end <- Sys.time()
# run2 <- end- start
# pick.end('007', 'ac')
```


```{r functions_csv_creation}

# make functions to look up starting and ending date of
# energy cluster period based on home
 pick.start <- function(x_home, x_cluster) {
   if(x_cluster == 'all') {
        omni_hourly %>%
   filter(home == x_home) %>%
   pull(datehour) %>% min()%>% date() %>% as.character()
   } else if( x_cluster %in% c('heat', 'shoulder', 'ac')){
   energy_cluster_df %>%
   filter(home == x_home & cluster_type == x_cluster) %>%
   pull(start_date) %>% as.character()
   } else if ( x_cluster %in% month.abb[1:12]){
       year <- if(x_cluster %in% month.abb[8:12]) '2020' else '2021'

              as.character(ymd(paste(year, x_cluster, '15')) %>%
                             floor_date('month'))

     }
 }

  pick.end <- function(x_home, x_cluster) {
    if(x_cluster == 'all') {
        omni_hourly %>%
   filter(home == x_home) %>%
   pull(datehour) %>% max()%>% date() %>% as.character()
   } else if( x_cluster %in% c('heat', 'shoulder', 'ac')){
   energy_cluster_df %>%
   filter(home == x_home & cluster_type == x_cluster) %>%
   pull(end_date) %>% as.character()
   } else if ( x_cluster %in% month.abb[1:12]){
       year <- if(x_cluster %in% month.abb[8:12]) '2020' else '2021'

              as.character(ymd(paste(year, x_cluster, '15')) %>%
                             ceiling_date('month') -days(1))

     }
  }
  
  # fun to pick minimum sample length to test based on metric
  pick.sample.min <- function(x_metric) {
    lag_summary_df %>%
      # use the pooled median of all clusters
      filter(metric == x_metric, energy_cluster == 'total') %>%
      pull(median)
  }
  
  # # test functions
   # pick.start('006', 'heat')
  # pick.end('004', 'heat')
      # pick.start('001', 'Dec')
      #       pick.end('001', 'Jan')


```


### Hourly Data

```{r data_maker_hourly, eval = FALSE}
# test function for multiple homes-----------------
   test_homes <- c('007', '012')
    test_cluster <- c('ac') # can do multiple
    mets <- c('pm25', 'voc')
    locs <- locations_indoor
    
    test<- lapply(locs, function(x_location) {
      
      lapply(mets, function(x_metric) {
        
        lapply(test_cluster, function(cluster_type) {
          
          lapply(test_homes, function(x_home) {
            
            start_date <- pick.start(x_home, cluster_type)
            end_date <- pick.end(x_home, cluster_type)
            sample_min <- pick.sample.min(met)
            sample_max <- if_else(
              as.numeric(as.Date(end_date) - as.Date(start_date))>=28,
              28,
              as.numeric(as.Date(end_date) - as.Date(start_date))
            )
            
            entropy.table.hourly(
              x_home, metric = x_metric,
              date_ranges_entire = list(c(start_date,
                                          end_date)),
              tested_sample_sequence = 
                c(1, seq(3,27,4)),
              location_type = x_location,
              all_time = all_time_choice) %>%
              
              # add in column for energy_cluster
              mutate(energy_cluster = cluster_type) 
          }
          ) %>%
            bind_rows()
        }
        ) %>%
          bind_rows()
      }
      ) %>%
        bind_rows()
    }
    ) %>%
      bind_rows()
    
    
      # make dfs--------------------
all_time_choice <- TRUE
    overlap_choice <- FALSE
# clust <- c('all','heat', 'shoulder', 'ac') # can do multiple
clust <- clusters_all

met <- 'co2'

loc <- 'living'
lapply(clust, function(cluster_type) {
  
  rep_data <-  lapply(homes_all, function(x_home) {
    
    start_date <- pick.start(x_home, cluster_type)
    end_date <- pick.end(x_home, cluster_type)
    sample_min <- pick.sample.min(met)
    sample_max <- if_else(
      as.numeric(as.Date(end_date) - as.Date(start_date))>=28,
      28,
      as.numeric(as.Date(end_date) - as.Date(start_date))
    )
    
    entropy.table.hourly(
      x_home, metric = met,
      date_ranges_entire = list(c(start_date,
                                  end_date)),
      tested_sample_sequence =
        c(c(1:7), 10, 12, 14,17,21,24,28),
      location_type = loc,
        overlap = overlap_choice,
      all_time = all_time_choice) %>%
      
      # add in column for energy_cluster
      mutate(energy_cluster = cluster_type) 
  }
  ) %>%
    bind_rows()
  
  # give a variable name to the created data
  assign(paste(met, loc, cluster_type, 'rep_data_time_hourly_pdf', sep = '_'),
         rep_data,
         envir = .GlobalEnv)
}
)
loc <- 'kitchen'
lapply(clust, function(cluster_type) {
  
  rep_data <-  lapply(homes_all, function(x_home) {
    
    start_date <- pick.start(x_home, cluster_type)
    end_date <- pick.end(x_home, cluster_type)
    sample_min <- pick.sample.min(met)
    sample_max <- if_else(
      as.numeric(as.Date(end_date) - as.Date(start_date))>=28,
      28,
      as.numeric(as.Date(end_date) - as.Date(start_date))
    )
    
    entropy.table.hourly(
      x_home, metric = met,
      date_ranges_entire = list(c(start_date,
                                  end_date)),
      tested_sample_sequence =
        c(c(1:7), 10, 12, 14,17,21,24,28),
      location_type = loc,
        overlap = overlap_choice,
      all_time = all_time_choice) %>%
      
      # add in column for energy_cluster
      mutate(energy_cluster = cluster_type) 
  }
  ) %>%
    bind_rows()
  
  # give a variable name to the created data
  assign(paste(met, loc, cluster_type, 'rep_data_time_hourly_pdf', sep = '_'),
         rep_data,
         envir = .GlobalEnv)
}
)
loc <- 'bedroom'
lapply(clust, function(cluster_type) {
  
  rep_data <-  lapply(homes_all, function(x_home) {
    
    start_date <- pick.start(x_home, cluster_type)
    end_date <- pick.end(x_home, cluster_type)
    sample_min <- pick.sample.min(met)
    sample_max <- if_else(
      as.numeric(as.Date(end_date) - as.Date(start_date))>=28,
      28,
      as.numeric(as.Date(end_date) - as.Date(start_date))
    )
    
    entropy.table.hourly(
      x_home, metric = met,
      date_ranges_entire = list(c(start_date,
                                  end_date)),
      tested_sample_sequence =
        c(c(1:7), 10, 12, 14,17,21,24,28),
      location_type = loc,
        overlap = overlap_choice,
      all_time = all_time_choice) %>%
      
      # add in column for energy_cluster
      mutate(energy_cluster = cluster_type) 
  }
  ) %>%
    bind_rows()
  
  # give a variable name to the created data
  assign(paste(met, loc, cluster_type, 'rep_data_time_hourly_pdf', sep = '_'),
         rep_data,
         envir = .GlobalEnv)
}
)
# loc <- 'pooled'
# lapply(clust, function(cluster_type) {
#   
#   rep_data <-  lapply(homes_all, function(x_home) {
#     
#     start_date <- pick.start(x_home, cluster_type)
#     end_date <- pick.end(x_home, cluster_type)
#     sample_min <- pick.sample.min(met)
#     sample_max <- if_else(
#       as.numeric(as.Date(end_date) - as.Date(start_date))>=28,
#       28,
#       as.numeric(as.Date(end_date) - as.Date(start_date))
#     )
#     
#     entropy.table.hourly(
#       x_home, metric = met,
#       date_ranges_entire = list(c(start_date,
#                                   end_date)),
#       tested_sample_sequence =
#         c(c(1:7), 9, seq(11,27,4)),
#       location_type = loc,
#         overlap = overlap_choice,
#       all_time = all_time_choice) %>%
#       
#       # add in column for energy_cluster
#       mutate(energy_cluster = cluster_type) 
#   }
#   ) %>%
#     bind_rows()
#   
#   # give a variable name to the created data
#   assign(paste(met, loc, cluster_type, 'rep_data_time_hourly_pdf', sep = '_'),
#          rep_data,
#          envir = .GlobalEnv)
# }
# )

  
entropy_time_hourly_pdf_df <- bind_rows(

co2_bedroom_heat_rep_data_time_hourly_pdf,
  co2_living_heat_rep_data_time_hourly_pdf,
  co2_kitchen_heat_rep_data_time_hourly_pdf,
  co2_bedroom_shoulder_rep_data_time_hourly_pdf,
  co2_living_shoulder_rep_data_time_hourly_pdf,
  co2_kitchen_shoulder_rep_data_time_hourly_pdf,
  co2_bedroom_ac_rep_data_time_hourly_pdf,
  co2_living_ac_rep_data_time_hourly_pdf,
  co2_kitchen_ac_rep_data_time_hourly_pdf
  # co2_bedroom_all_rep_data_time_hourly_pdf,
  # co2_living_all_rep_data_time_hourly_pdf,
  # co2_kitchen_all_rep_data_time_hourly_pdf
)



# # if did months
# entropy_time_hourly_pdf_df <-
#   map(c('pm25', 'voc'), function(x_metric) {
#     map(month.abb[1:12], function(x_cluster) {
#       map(locations_indoor, function(x_location){
#         get(paste(x_metric, x_location, x_cluster, 'rep_data_time_hourly_pdf', sep = '_'))
#       }) %>% bind_rows()
#     }) %>% bind_rows()
#   }) %>% bind_rows()



# if did pooled
entropy_time_hourly_pdf_df_pooled <- bind_rows(
  pm25_pooled_heat_rep_data_time_hourly_pdf,
  pm25_pooled_shoulder_rep_data_time_hourly_pdf,
  pm25_pooled_ac_rep_data_time_hourly_pdf,
  pm25_pooled_all_rep_data_time_hourly_pdf,
  voc_pooled_heat_rep_data_time_hourly_pdf,
  voc_pooled_shoulder_rep_data_time_hourly_pdf,
  voc_pooled_ac_rep_data_time_hourly_pdf,
  voc_pooled_all_rep_data_time_hourly_pdf
)
# rm(
#   pm25_pooled_heat_rep_data_time_hourly_pdf,
#   pm25_pooled_shoulder_rep_data_time_hourly_pdf,
#   pm25_pooled_ac_rep_data_time_hourly_pdf,
#   pm25_pooled_all_rep_data_time_hourly_pdf,
#   voc_pooled_heat_rep_data_time_hourly_pdf,
#   voc_pooled_shoulder_rep_data_time_hourly_pdf,
#   voc_pooled_ac_rep_data_time_hourly_pdf,
#   voc_pooled_all_rep_data_time_hourly_pdf,
#   entropy_time_hourly_pdf_df
#   )
# rm(
#   pm25_bedroom_heat_rep_data_time_hourly_pdf,
#   pm25_living_heat_rep_data_time_hourly_pdf,
#   pm25_kitchen_heat_rep_data_time_hourly_pdf,
#   pm25_bedroom_shoulder_rep_data_time_hourly_pdf,
#   pm25_living_shoulder_rep_data_time_hourly_pdf,
#   pm25_kitchen_shoulder_rep_data_time_hourly_pdf,
#   pm25_bedroom_ac_rep_data_time_hourly_pdf,
#   pm25_living_ac_rep_data_time_hourly_pdf,
#   pm25_kitchen_ac_rep_data_time_hourly_pdf,
#   voc_bedroom_heat_rep_data_time_hourly_pdf,
#   voc_living_heat_rep_data_time_hourly_pdf,
#   voc_kitchen_heat_rep_data_time_hourly_pdf,
#   voc_bedroom_shoulder_rep_data_time_hourly_pdf,
#   voc_living_shoulder_rep_data_time_hourly_pdf,
#   voc_kitchen_shoulder_rep_data_time_hourly_pdf,
#   voc_bedroom_ac_rep_data_time_hourly_pdf,
#   voc_living_ac_rep_data_time_hourly_pdf,
#   voc_kitchen_ac_rep_data_time_hourly_pdf,
#   entropy_time_hourly_pdf_df
# )
# write csv files------------------------




# if not pooled ---------------------
write_rds(entropy_time_hourly_pdf_df, file =
            paste0('./csv_created/representativeness_data/rep_data_time_hourly_pdf',
                   ifelse(all_time_choice == TRUE, '', '_season'),
                   ifelse(overlap_choice == TRUE, '', '_no_overlap'),
                   ifelse(clust[1] %in% month.abb[1:12], '_month', ''),
                   '.rds'))
# make dated backup
write_rds(entropy_time_hourly_pdf_df, file =
            paste0('./csv_created/representativeness_data/rep_data_time_hourly_pdf_',
                   Sys.Date(),
                   ifelse(all_time_choice == TRUE, '', '_season'),
                   ifelse(overlap_choice == TRUE, '', '_no_overlap'),
                   ifelse(clust[1] %in% month.abb[1:12], '_month', ''),
                   '.rds'))
# if pooled--------
write_rds(entropy_time_hourly_pdf_df_pooled, file =
            paste0('./csv_created/representativeness_data/rep_data_time_hourly_pdf_pooled',
                   ifelse(all_time_choice == TRUE, '', '_season'),
                   ifelse(overlap_choice == TRUE, '', '_no_overlap'),
                   ifelse(clust[1] %in% month.abb[1:12], '_month', ''),
                   '.rds'))
# make dated backup
write_rds(entropy_time_hourly_pdf_df_pooled, file =
            paste0('./csv_created/representativeness_data/rep_data_time_hourly_pdf_pooled_',
                   Sys.Date(),
                   ifelse(all_time_choice == TRUE, '', '_season'),
                   ifelse(overlap_choice == TRUE, '', '_no_overlap'),
                   ifelse(clust[1] %in% month.abb[1:12], '_month', ''),
                   '.rds'))
```

